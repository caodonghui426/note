> > 1. 理论《深度学习》
> 2. Pytorch文档
> 3. 复线经典论文：读代码<===>写代码
> 4. 拓展视野

# 1.线性神经网络
在向量空间中，轴0代表列，轴1代表行。
> 运行流程：
> 1. 构建数据集（数据迭代器）
> 2. 定义模型、初始化模型参数、定义损失、定义优化算法
> 3. 训练：
3.1 定义超参数：学习率lr、重复次数epoch
3.2 指定模型net、指定损失loss
3.3 外层epoch循环
3.3.1 内层数据迭代器循环
3.3.1.1 前向传播（从输入层开始，通过神经网络进行前向传播计算，将输入数据从输入层传递到输出层，得到模型的预测输出。）
3.3.1.2 计算损失loss（将预测输出与真实标签进行比较，计算损失函数的值，衡量模型预测的误差。）
3.3.1.3 优化器梯度清零
3.3.1.4 反向传播：通过loss求backward，通过链式法则（链式求导）来计算损失函数关于模型参数的梯度。这个过程从输出层开始，将梯度从后向前传递，直到达到输入层。反向传播计算得到了每个参数对于损失函数的梯度。）

## 1.线性模型
线性模型是唯一具有最优解的模型。
### 1.1 基础优化方法
梯度下降通过不断沿着反梯度方向更新参数求解，小批量随机梯度下降是深度学习默认的求解算法。
超参数的设置：

1. 学习率：太大 --> 收敛困难，训练过程中抖动比较大；太小 --> 收敛速度过慢，增大计算代价。可能停滞在局部最优，无法找到全局最优。
2. 批量大小：太大 --> 内存消耗增加，浪费计算；太小 --> 导致每次计算量太小，不适合并行来最大利用计算资源。
# 2.Logistic Regression 
Logistic函数可以将实数区间 ℝ 映射到 [0, 1]区间内，如下图所示。所以Logistic可以用于表示概率问题，并且在深度学习中Logistic函数通常被称为Sigmoid函数。其他常见的Sigmoid函数如下图所示。
![CleanShot 2023-08-19 at 22.24.12@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1692455069422-ad56e74a-a6b7-45ec-b9d0-55f2f119c6eb.png#averageHue=%23faf9f9&clientId=u621f7aeb-908a-4&from=paste&height=358&id=VRs3D&originHeight=820&originWidth=1112&originalType=binary&ratio=2&rotation=0&showTitle=false&size=111974&status=done&style=none&taskId=uf8f5c5fb-dbb1-42a7-9382-3985b952af0&title=&width=485)
$\sigma(x)=\frac{1}{1+e^{-x}}$
![CleanShot 2023-08-19 at 22.29.34@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1692455390655-9e39a66b-1a97-4c25-92ff-40792f892cbe.png#averageHue=%23fafaf9&clientId=u621f7aeb-908a-4&from=paste&height=352&id=u8d47cb86&originHeight=1036&originWidth=2146&originalType=binary&ratio=2&rotation=0&showTitle=false&size=301890&status=done&style=none&taskId=u974d7ed7-d950-4371-b3ad-9c60b884b53&title=&width=730)
`BCELoss`是二元交叉熵损失函数（Binary Cross-Entropy Loss）的缩写，用于衡量二分类问题中模型预测和真实标签之间的差异。在深度学习中，特别是在二分类任务（如图像是否包含某个对象、邮件是否是垃圾邮件等）中，BCELoss 经常用于训练模型，以便模型能够更好地逼近真实的标签。
![CleanShot 2023-08-19 at 22.36.36@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1692455816075-e48b0bd4-0186-4bbf-8c1d-004213d0c61b.png#averageHue=%23ebd9d1&clientId=u621f7aeb-908a-4&from=paste&height=232&id=u446c7725&originHeight=632&originWidth=1228&originalType=binary&ratio=2&rotation=0&showTitle=false&size=76485&status=done&style=none&taskId=uc65d2054-40ac-47d2-a4ac-379aa4168fb&title=&width=450)
通常在实际代码中，这个损失函数会结合激活函数，如 Sigmoid，来将模型的输出映射到 [0, 1] 范围内的概率值。在 PyTorch 中，可以通过 **torch.nn.BCELoss()** 函数来创建 **BCELoss** 损失函数，并结合 Sigmoid 激活函数。
Logistic的实现：
![CleanShot 2023-08-19 at 22.42.52@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1692456212226-e0b5bfee-5861-4b69-9ad3-0ec3f61002ec.png#averageHue=%23f3ede3&clientId=u621f7aeb-908a-4&from=paste&height=537&id=ub4b8df9d&originHeight=1074&originWidth=2268&originalType=binary&ratio=2&rotation=0&showTitle=false&size=352643&status=done&style=none&taskId=uf8cbef89-8f43-482c-b307-9b18d77d59f&title=&width=1134)
# 3.SoftMax回归

- softmax运算获取一个向量并将其映射为概率。
- softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。
- 交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数。
## 3.1回归与分类的区别

- 回归估计一个连续值： 
   - 单连续值作为输出
   - 自然区间 ℝ
   - 跟真实值的区别作为损失
- 分类预测一个离散类别 
   - 通常多个输出
   - 输出 $i$ 是预测为第 $i$ 类的置信度
## 3.2从回归到多分类-校验比例

- 输出匹配概率（非负，和为1）
$\hat{y}=softmax(o)$，$\hat{y_i}=\frac{exp(o_i)}{\sum_k exp(o_k)}$
- 概率 $y$ 和 $\hat{y}$ 的区别作为损失
## 3.3 Softmax和交叉熵损失

- 交叉熵损失常用来衡量两个概率的区别 $H(p,q)=\sum_i-p_ilog(q_i)$
- 将它作为损失（因为 $y$ 是one-hot编码，所以只有第 $y_y$ 个元素为1（表示真实类别），其余元素都为0。因此，在求和符号中，只有一个项参与求和，即 $i=y_y$ 的项）
$l(y,\hat{y})=-\sum_i y_i log \hat{y_i}=-log\hat{y_y}$
- 其梯度是真实概率和预测概率的区别
$\partial_{o_i}l((y,\hat{y})=softmax(o)_i-y_i$



# 4.卷积神经网络
## 4.1 Basic CNN
卷积神经网络可以保存空间信息，它的设计灵感来自于生物视觉系统中的视觉皮层处理方式，可以有效地捕捉图像中的空间特征和模式。CNN在处理二维数据（如图像）时，通过卷积层、池化层和全连接层等组件，逐层提取和学习特征，然后用于分类、定位、分割等任务。以下是CNN的主要组件和工作原理：

1. **卷积层（Convolutional Layers）**：卷积操作是CNN的核心。卷积层通过使用卷积核（也称为过滤器）在输入图像上滑动，逐步计算出特征图。卷积操作可以捕捉局部特征，如边缘、纹理等。通过堆叠多个卷积层，网络可以学习更抽象的特征。
2. **激活函数（Activation Function）**：在卷积层之后，会应用一个激活函数，如ReLU（Rectified Linear Activation），以引入非线性性。这有助于网络学习复杂的非线性特征。
3. **池化层（Pooling Layers）**：池化操作用于减少特征图的尺寸，从而降低网络的计算量和参数数量，同时保留重要的特征。最常见的池化操作是最大池化，它选取池化区域内的最大值作为池化后的值。
4. **全连接层（Fully Connected Layers）**：在经过一系列卷积和池化操作后，通常会将特征图展平，并通过全连接层进行分类或其他任务。全连接层将提取到的特征映射到不同的类别。
5. **优化器和损失函数**：训练CNN时，使用优化器来更新网络参数以最小化定义的损失函数。常见的优化器包括梯度下降法的变种，如Adam、SGD等。损失函数根据任务类型不同，可以是交叉熵、均方误差等。

CNN的训练过程涉及将输入数据通过前向传播推送至网络，然后使用反向传播计算梯度并更新权重，从而逐渐提升网络性能。在大规模数据集上进行训练，CNN可以学习到更高级别的特征表示，从而在图像分类、目标检测、图像分割等任务中表现出色。
![CleanShot 2023-08-28 at 22.04.50@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693231505349-fc967610-d0e8-4534-95d3-164cf6c19606.png#averageHue=%23f3efee&clientId=u394b6979-5614-4&from=paste&height=507&id=u7f456068&originHeight=1014&originWidth=2306&originalType=binary&ratio=2&rotation=0&showTitle=false&size=171785&status=done&style=none&taskId=u1954d0b1-c8b6-459e-a145-2f76bef671b&title=&width=1153)
在下面的例子中，输入的batch为1，通道为1，w和h为28，经过第一个卷积层，输出通道数为10，卷积核大小为5x5，那么卷积层的通道为10，w和h由于卷积核的原因，各减去2x2=4得24x24，因为卷积核中心距离边的长度为2。经过2x2的池化层，通道数不变，w和h减半。后面以此类推。
![CleanShot 2023-08-28 at 22.14.50@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693232099837-5701d904-fe54-427d-9afe-131c3e1416d5.png#averageHue=%23f3f2f2&clientId=u394b6979-5614-4&from=paste&height=463&id=u4a0525aa&originHeight=926&originWidth=2294&originalType=binary&ratio=2&rotation=0&showTitle=false&size=172041&status=done&style=none&taskId=u9002c3a6-a7b2-43ad-bdbb-72f1cfb71bf&title=&width=1147)
输入的通道数为n，输出的通道数可能会发生改变，方法是提供m个卷积核，然后将所有卷积过的Feature map进行Cat，得到$(m,width_{out},height_{out})$矩阵。所以需要提供四个参数，分别是输入通道大小n，输出通道大小m，以及卷积核大小 w 和 h。
![CleanShot 2023-08-28 at 22.21.01@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693232470315-a3b6576a-f152-46c8-977b-46e7f89cd304.png#averageHue=%23f7f4f2&clientId=u394b6979-5614-4&from=paste&height=498&id=u40c1dc5e&originHeight=996&originWidth=2176&originalType=binary&ratio=2&rotation=0&showTitle=false&size=198070&status=done&style=none&taskId=ud5c16582-97f6-44cc-a892-abb5d8ccc26&title=&width=1088)
![CleanShot 2023-08-28 at 22.21.23@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693232493733-5fc80ecc-4990-424d-bf43-000a5e57cf0c.png#averageHue=%23f3ebe9&clientId=u394b6979-5614-4&from=paste&height=468&id=u6adbd8da&originHeight=936&originWidth=2312&originalType=binary&ratio=2&rotation=0&showTitle=false&size=152571&status=done&style=none&taskId=u8e0d23f6-3af5-4374-a90f-8b2e8dff91e&title=&width=1156)
完整的卷积神经网络如下，在训练的过程中使用GPU加速：						
```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") 
model.to(device) 
```

![CleanShot 2023-08-28 at 22.29.29@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693232982111-a72543dc-cd69-4b9a-8b9b-95893bfc3c6d.png#averageHue=%23f3ece2&clientId=u394b6979-5614-4&from=paste&height=535&id=u6cb19414&originHeight=1070&originWidth=2306&originalType=binary&ratio=2&rotation=0&showTitle=false&size=407956&status=done&style=none&taskId=u6643b705-6a17-463a-9771-05296d86448&title=&width=1153)
![CleanShot 2023-08-28 at 22.32.00@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693233131150-626f0476-3548-4dd0-a911-ef4c2e44d661.png#averageHue=%23efdecc&clientId=u394b6979-5614-4&from=paste&height=398&id=ufc577e28&originHeight=796&originWidth=1806&originalType=binary&ratio=2&rotation=0&showTitle=false&size=217956&status=done&style=none&taskId=udde5338b-7241-451d-9ea1-acc37a05b1c&title=&width=903)
![CleanShot 2023-08-28 at 22.32.15@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693233145959-4a6c53a2-d4fc-4728-b45f-9583a85cd94c.png#averageHue=%23ebd5c2&clientId=u394b6979-5614-4&from=paste&height=287&id=u7dfbcc98&originHeight=574&originWidth=1790&originalType=binary&ratio=2&rotation=0&showTitle=false&size=176116&status=done&style=none&taskId=uf3323b24-b3c3-48c3-9925-4a7e285ee68&title=&width=895)
## 4.2 Advanced CNN
### 4.2.1 GoogLeNet
GoogleNet，也称为Inception v1，是由Google团队提出的深度卷积神经网络架构，旨在解决深层神经网络训练中的梯度消失和计算量增加等问题。GoogleNet引入了"Inception"模块，这是一种多尺度的特征提取方法，可以在不同尺度下同时捕获不同层次的特征。
![CleanShot 2023-08-29 at 15.33.55@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693294448007-9ac76952-5df3-4505-80e9-d99af699e61e.png#averageHue=%23fbf7f6&clientId=u8449917d-f28e-4&from=paste&height=452&id=ub86b5e39&originHeight=904&originWidth=2080&originalType=binary&ratio=2&rotation=0&showTitle=false&size=969122&status=done&style=none&taskId=u1a8af044-1e98-4787-a0dd-f1911e0c597&title=&width=1040)
GoogleNet的设计思想是在保持计算效率的同时，增加网络的宽度和深度，从而提高网络的表达能力。其中最重要的是"Inception"模块，这是一个由多个并行卷积操作和池化操作组成的结构，以并行方式捕获不同尺度的特征，并将它们串联在一起。每个"Inception"模块内部的操作可以被视为网络在不同感受野下进行特征提取。
"Inception"模块中的卷积操作和池化操作可以同时工作，以不同的尺度处理输入数据。这种并行的方式可以更好地捕获不同大小的特征，使网络在图像中的多个层次上都能提取有用的信息。在最后，所有并行分支的输出被连接在一起，形成了一个更丰富的特征表示。
GoogleNet还引入了1x1的卷积核，称为"瓶颈层"，用于降低网络中的计算量。这些1x1的卷积核可以在不改变特征图大小的情况下，将特征通道的维度减小，从而减少网络参数的数量。
![CleanShot 2023-08-29 at 15.45.13@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693295127772-4405d55e-28f4-4e33-b1ea-54469e987d33.png#averageHue=%23faf8f5&clientId=u8449917d-f28e-4&from=paste&height=394&id=u126baf58&originHeight=1010&originWidth=1124&originalType=binary&ratio=2&rotation=0&showTitle=false&size=114141&status=done&style=none&taskId=uf08f460c-d9df-4c50-a09e-8988c9fa708&title=&width=438)
GoogleNet还引入了1x1的卷积核，称为"瓶颈层"，用于降低网络中的计算量。这些1x1的卷积核可以在不改变特征图大小的情况下，将特征通道的维度减小，从而减少网络参数的数量。
![CleanShot 2023-08-29 at 15.51.20@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693295493878-31bb6175-c9b7-4473-8ce9-81cfcd255eeb.png#averageHue=%23f7f5f4&clientId=u8449917d-f28e-4&from=paste&height=364&id=u89f372c6&originHeight=1044&originWidth=1934&originalType=binary&ratio=2&rotation=0&showTitle=false&size=169731&status=done&style=none&taskId=u4d0f282b-25c4-49ea-ba65-bb5e6ce47a7&title=&width=675)
Inception模块的实现：
![CleanShot 2023-08-29 at 15.53.04@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693295602749-ae82d29f-71e2-449d-a329-48c6262c88d9.png#averageHue=%23f4f0e7&clientId=u8449917d-f28e-4&from=paste&height=535&id=ub6e951e5&originHeight=1070&originWidth=2176&originalType=binary&ratio=2&rotation=0&showTitle=false&size=384985&status=done&style=none&taskId=u10cd216c-42b2-4e95-970e-83d813bd581&title=&width=1088)
![CleanShot 2023-08-29 at 15.57.10@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693295844687-98c05436-ad43-4f42-be91-9d26e15d6a1f.png#averageHue=%23f8f5f1&clientId=u8449917d-f28e-4&from=paste&height=528&id=ubc2d5aa4&originHeight=1056&originWidth=2312&originalType=binary&ratio=2&rotation=0&showTitle=false&size=228368&status=done&style=none&taskId=u4004233f-c446-42b4-826c-7a19e6b942e&title=&width=1156)
![CleanShot 2023-08-29 at 15.58.06@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693295896131-dae901de-e37a-4d33-803f-32eda407ec12.png#averageHue=%23f9f4e2&clientId=u8449917d-f28e-4&from=paste&height=531&id=ufe1b0791&originHeight=1062&originWidth=2298&originalType=binary&ratio=2&rotation=0&showTitle=false&size=516712&status=done&style=none&taskId=u5965315a-1dbf-44e7-82bc-f227256ec3f&title=&width=1149)
### 4.2.2 Residual Network（ResNet）
> 1. **梯度消失（Gradient Vanishing）**：梯度消失指的是在反向传播过程中，网络的深层层次的梯度逐渐变得非常小，甚至趋近于零。这意味着在更新网络参数时，深层的神经元几乎没有更新，导致它们无法学习到有意义的特征。梯度消失通常出现在网络层数很深，激活函数使用导数较小的情况下，例如使用Sigmoid激活函数时。
> 2. **梯度爆炸（Gradient Explosion）**：梯度爆炸则是与梯度消失相反的情况。在反向传播时，网络的梯度可能会变得非常大，导致权重更新过大，从而让网络的参数值迅速增大。这可能会导致数值不稳定性，使网络的输出变得异常或溢出。
> 
梯度消失和梯度爆炸都会阻碍神经网络的训练过程。当梯度消失发生时，网络无法从数据中有效地学习特征，而梯度爆炸可能会导致网络无法稳定地进行参数更新。这些问题会导致网络的训练过程变得非常缓慢甚至失败。
> 为了解决这些问题，研究人员提出了许多方法：
> - **激活函数的选择**：使用Rectified Linear Unit（ReLU）等激活函数可以缓解梯度消失问题，因为它在正区间上的导数为1，避免了导数过小的情况。
> - **权重初始化**：使用适当的权重初始化方法可以帮助避免梯度爆炸。例如，Xavier初始化和He初始化方法可以使权重在不同层之间保持适当的尺度。
> - **Batch Normalization**：批归一化可以在网络的每个层之后对输出进行归一化，从而有助于减少梯度消失和加速收敛。
> - **梯度裁剪**：梯度裁剪技术可以限制梯度的范围，防止梯度爆炸。
> - **Skip Connections**：通过引入跳跃连接（如ResNet中的残差连接），可以帮助梯度在网络中流动更顺畅，缓解梯度消失问题。

残差网络（Residual Network，通常简称为ResNet）是由Kaiming He等人于2015年提出的深度神经网络架构，用于解决深层神经网络训练中的梯度消失和网络退化等问题。ResNet引入了残差块（Residual Block）的概念，允许网络通过跳跃连接（或称为残差连接）来学习残差映射，从而更容易地训练非常深的网络。
![He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2016:770-778.](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693296720443-a80a7870-b3a2-421e-a1f2-d54b912260b1.png#averageHue=%23fcfbfb&clientId=u8449917d-f28e-4&from=paste&height=241&id=uf76829c4&originHeight=768&originWidth=1838&originalType=binary&ratio=2&rotation=0&showTitle=true&size=274042&status=done&style=none&taskId=u0b5ecb6c-9168-4918-8403-8464a25354d&title=He%20K%2C%20Zhang%20X%2C%20Ren%20S%2C%20et%20al.%20Deep%20Residual%20Learning%20for%20Image%20Recognition%5BC%5D%2F%2F%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition.%20IEEE%20Computer%20Society%2C%202016%3A770-778.&width=577 "He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2016:770-778.")
在传统的神经网络中，层叠多个卷积层可能会导致梯度消失，从而阻碍训练过程。ResNet通过在每个残差块中引入跳跃连接来解决这个问题。跳跃连接将输入数据绕过一些卷积层，直接传递到后续的层，这样网络可以学习捕捉残差，即输入和期望输出之间的差异。
一个典型的残差块包含了两个主要分支：

1. **主要分支（Identity Branch）**：这个分支通过卷积层对输入进行变换。这些卷积层的目标是学习残差的变换，使得输入数据与期望输出之间的差异能够被捕捉。
2. **跳跃连接分支（Shortcut Connection）**：这个分支直接将输入数据绕过一些卷积层，传递到后续的层。这样，网络可以选择性地学习残差，而不是必须从头开始学习整个映射。

通过将这两个分支相加，得到最终的残差块输出。这使得网络在训练过程中可以更容易地学习残差信息，使得深层网络能够更稳定地收敛。
![CleanShot 2023-08-29 at 16.14.55@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1693296913774-bcbc3a3c-7e86-41e3-ad64-d82afdcc75f8.png#averageHue=%23f6f4ec&clientId=u8449917d-f28e-4&from=paste&height=425&id=u543d5f5b&originHeight=850&originWidth=2202&originalType=binary&ratio=2&rotation=0&showTitle=false&size=242210&status=done&style=none&taskId=u733016c1-e75a-468a-a492-8732ba827f3&title=&width=1101)
ResNet的一大贡献是证明了深层神经网络不一定导致性能下降，反而可以通过适当的设计和跳跃连接来提高网络性能。事实上，更深的ResNet通常能够获得更好的性能，这种观点推动了后续更深层次的网络设计。ResNet在图像分类、目标检测等任务中取得了显著的性能提升，并对深度神经网络的研究产生了深远影响。

- [ ] 复现[Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)
- [ ] 复现[Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993v5)
# A. 论文
## (Vit)An IMAGE IS WORTH 16x16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE
### 摘要
虽然Transformers架构已经成为自然语言处理任务的一个标准，到那时它在计算机视觉上的应用仍然有限。在视觉上，注意力要么是与卷积网络结合使用，要么是被用于替换卷机网络中的一些特定的组件，在这一过程中保持整体结构不变。我们表明，这种对于CNN的依赖并不是必须的，**直接应用于图像patch（块）序列的纯Transformer可以在图像分类任务上表现得非常好**。当对大量数据进行预训练之后并将其转移到多个中小型图像识别基准（ImageNet，CIFAR-100，VTAB，等）时，**与state-of-the-art卷积网络相比需要少的训练资源**。「这里打成patch（块）的原因是 **每个像素都与其他所有像素计算注意力,那么计算量会非常大**」
> 比如一张图片的宽度和高度是$224\times{224}$，如果按照像素把它拉平，序列的长度

### 导言
基于自注意力架构，尤其是Transformers，已成为自然语言处理（NLP）的首选模型。主要的方法是在大型文本语料库上进行预训练，然后在较小的特定任务数据集上进行微调。由于 Transformers 的计算效率和可扩展性，训练具有超过100B参数的模型成为可能。**随着模型和数据集的增长，仍然没有饱和性能的迹象。**
然而在计算机视觉中，卷积架构仍然占主导地位。受 NLP 成功的启发，多项工作尝试将类似 CNN 的架构与自注意结合，其中一些完全取代了卷积。后一种模型虽然在理论上是有效的，但由于使用了专门的注意力模式，尚未在现代硬件加速器上有效地扩展。因此，在大规模图像识别中，经典的类 ResNet 架构仍然是SOTA模型。
受NLP中Transformer成功的启发，我们尝试将标准Transformer直接应用于图像，尽可能少地进行修改。为此，我们将图像分割成块（**patch**），**并提供这些块的线性嵌入序列作为Transformer的输入**。图像块的处理方式与被用于NLP中的tokens（words）相同。**我们以监督方式训练模型进行图像分类**。
当在没有强正则化的ImageNet等中型数据集上进行训练时，这些模型产生的准确度比同等大小的ResNet 低几个百分点。这种看似令人沮丧的结果可能是意料之中的：Transformer缺乏CNN固有的一些归纳偏置，比如平移不变性和局部性。因此在数据量不足的情况下不能很好的泛化。
但是，如果模型在更大的数据集（14M-300M 图像）上训练，情况就会发生变化。我们发现大规模训练胜过归纳偏置。我们的 Vision Transformer (ViT) 在以足够的规模进行**预训练**并转移到具有**较少数据点**的任务时获得了出色的结果。当在公共 ImageNet-21k 数据集或内部 JFT-300M 数据集上进行预训练时，ViT 在多个图像识别基准上接近或超过了最先进的水平。特别是，最好的模型在 ImageNet上达到 88.55%，在 ImageNet-ReaL 上达到 90.72%，在 CIFAR-100 上达到 94.55%，在 19 个任务的VTAB 套件上达到 77.63%。
### 结论
我们已经探索了在图像识别中直接应用Transformer。在计算机视觉中与先前使用自注意力的工作不同的是，除了初始patch提取步骤之外，我们不会将图像特定的归纳偏置引入架构中。相反，我们将一张图像解释或表示为一个patch序列，然后用NLP中标准的Transformer encoder来处理这个序列。这种简单但可扩展的策略在与大型数据集的预训练相结合时效果出奇地好。因此，Vision Transformer 在许多图像分类数据集上匹配或超过了现有技术，同时预训练成本相对较低。
尽管这些初步结果令人鼓舞，但仍然存在许多挑战。一种是将 ViT 应用于其他计算机视觉任务，例如检测和分割。我们的结果，加上《End-to-End Object Detection with Transformers（DETR）》，表明了这种方法的前景。另一个挑战是继续探索自监督的预训练方法。我们最初的实验显示了自监督预训练的改进，但自监督和大规模监督预训练之间仍然存在很大差距。最后，进一步扩展 ViT 可能会提高性能。**（MAE）**
### 相关工作
Transformer是在2017年为机器翻译提出的，目前它已经成为许多NLP任务中最先进的方法。基于Transformer的大型模型通常在大规模语料上进行预训练，然后针对特定任务进行微调：BERT使用去噪的自监督预训练任务，而GPT系列工作则使用语言建模作为其预训练任务。
将自注意力直接应用于图像，需要每个像素都去关注其他每一个像素**（即每个像素都与其他所有像素计算注意力,那么计算量会非常大）**。随着像素数量的二次方增长，这不可扩展到实际的输入大小。因此，为了将Transformer应用于图像处理，过去曾尝试过几种近似方法。比如仅在每个查询像素的局部邻域而不是全局应用了自注意力。这样的局部多头点积自注意力块可以完全取代卷积。在另一条研究线中，稀疏Transformer采用可扩展的全局自注意力近似，以便应用于图像。另一种扩展注意力的方法是以不同大小的块应用它，在极端情况下仅沿单个轴。许多这些专用的注意力体系结构在计算机视觉任务上展现出有希望的结果，但需要复杂的工程来在硬件加速器上有效实现。
与我们的工作最相关的是Cordonnier等人(2020)的模型，它从输入图像中提取2×2的patch,在顶部应用完整的自注意力。这个模型与ViT非常相似，但是我们的工作进一步表明大规模预训练使普通transformer与目前最先进的CNN竞争(甚至更好)。此外，Cordonnier等(2020)使用了2×2像素的小patch大小，这使得该模型仅适用于小分辨率图像，而我们也可以处理中等分辨率图像。
还有大量的工作致力于将卷积神经网络(CNN)与某种形式的自注意力相结合，例如，通过增强特征映射进行图像分类，或通过使用自注意力进一步处理CNN的输出进行目标检测，视频处理，图像分类，无监督对象发现，或统一的文本视觉任务。
另一个最近相关的模型是图像GPT(iGPT)，它在降低图像分辨率和颜色空间后将Transformer应用于图像像素。该模型以无监督的方式作为生成模型进行训练，然后可以微调或线性探测结果表示以进行分类，在ImageNet上达到72%的最高准确率。我们的工作进一步丰富了探索大于标准ImageNet数据集规模的图像识别的日益增多的论文集合。使用额外的数据源使得可以在标准基准测试上达到最先进的结果。此外,Sun等人(2017)研究了CNN性能与数据集大小的扩展关系,而Kolesnikov等人(2020); Djolonga等人(2020)从像ImageNet-21k和JFT-300M这样的大规模数据集进行CNN迁移学习进行了经验探索。我们也关注这两个数据集,但训练的是Transformer而不是之前工作中使用的基于ResNet的模型。
### 模型
![模型概述。我们将一个图像分割成固定大小的patch,线性嵌入每个patch,添加位置嵌入,并将生成的向量序列馈送到标准的Transformer encoder中。为了进行分类,我们使用了将一个可学习的“分类token”添加到序列的标准方法。Transformer encoder的插图改编自Vaswani等(2017)。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1694568308133-302f179c-117c-45fa-9d72-0da5ecb9eabc.png#averageHue=%23ebe9e6&clientId=uc737b816-653a-4&from=paste&height=688&id=uddbbee7e&originHeight=1376&originWidth=2706&originalType=binary&ratio=2&rotation=0&showTitle=true&size=868367&status=done&style=none&taskId=u62f5886d-dfcd-4224-8552-5c3a1c06cc3&title=%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0%E3%80%82%E6%88%91%E4%BB%AC%E5%B0%86%E4%B8%80%E4%B8%AA%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%88%90%E5%9B%BA%E5%AE%9A%E5%A4%A7%E5%B0%8F%E7%9A%84patch%2C%E7%BA%BF%E6%80%A7%E5%B5%8C%E5%85%A5%E6%AF%8F%E4%B8%AApatch%2C%E6%B7%BB%E5%8A%A0%E4%BD%8D%E7%BD%AE%E5%B5%8C%E5%85%A5%2C%E5%B9%B6%E5%B0%86%E7%94%9F%E6%88%90%E7%9A%84%E5%90%91%E9%87%8F%E5%BA%8F%E5%88%97%E9%A6%88%E9%80%81%E5%88%B0%E6%A0%87%E5%87%86%E7%9A%84Transformer%20encoder%E4%B8%AD%E3%80%82%E4%B8%BA%E4%BA%86%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB%2C%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%E4%BA%86%E5%B0%86%E4%B8%80%E4%B8%AA%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E2%80%9C%E5%88%86%E7%B1%BBtoken%E2%80%9D%E6%B7%BB%E5%8A%A0%E5%88%B0%E5%BA%8F%E5%88%97%E7%9A%84%E6%A0%87%E5%87%86%E6%96%B9%E6%B3%95%E3%80%82Transformer%20encoder%E7%9A%84%E6%8F%92%E5%9B%BE%E6%94%B9%E7%BC%96%E8%87%AAVaswani%E7%AD%89%282017%29%E3%80%82&width=1353 "模型概述。我们将一个图像分割成固定大小的patch,线性嵌入每个patch,添加位置嵌入,并将生成的向量序列馈送到标准的Transformer encoder中。为了进行分类,我们使用了将一个可学习的“分类token”添加到序列的标准方法。Transformer encoder的插图改编自Vaswani等(2017)。")
![Untitled.drawio.svg](https://cdn.nlark.com/yuque/0/2023/svg/25721528/1694661268192-e5a2fe50-c48c-4928-b349-d0210cad8f6b.svg#clientId=u817e8951-8ab5-4&from=paste&height=998&id=u141ca081&originHeight=1134&originWidth=702&originalType=binary&ratio=2&rotation=0&showTitle=false&size=141162&status=done&style=none&taskId=u2b903f38-e315-45cf-8835-d6776cba730&title=&width=618)
标准的Transformer输入是一个1D的token嵌入序列。为处理2D图像，我们将图像$x\in\mathbb{R}^{{H}\times{W}\times{C}}$重塑为一个展平的2D patch序列$x_p\in{\mathbb{R}^{{N}\times{P^2\cdot{C}}}}$，其中$(H,W)$是原始图像的分辨率，$C$是通道数，$(P,P)$是每个图像patch的分辨率，$N=HW/P^2$是生成的patch数量，也是Transformer的有效输入序列长度。Transformer在所有层中使用固定的潜在向量大小$D$，所以我们展平patch并用一个可训练的线性投影映射到$D$维(公式1)。我们将这个投影的输出称为patch嵌入。
类似于BERT的[class] token，我们在嵌入的patch序列前添加一个可学习的嵌入$z_0^0 = x_{class}$,其在Transformer encoder输出的状态$(z_L^0)$用作图像表示$y$(公式4)。在预训练和微调期间,都在$z_L^0$上附加一个分类头。预训练时的分类头通过一个隐藏层的MLP实现,微调时则用单线性层实现。
为保留位置信息,我们在patch嵌入中添加位置嵌入。我们使用标准的1D可学习位置嵌入，因为我们没有观察到使用更高级的2D感知位置嵌入带来明显的性能改进(附录D.3)。生成的嵌入向量序列作为Transformer编码器的输入。
Transformer编码器由多头自注意力层和MLP块交替组成。 在每个块之前应用层归一化(LN)，在每个块之后应用残差连接。
MLP包含两个层,使用GELU非线性激活函数。
> MLP模块包含两层全连接网络,以及在两层全连接之间使用的非线性激活函数GELU。
> 具体来说:
> - 第一层是仿射变换(线性变换加偏置),如Wx+b
> - 然后应用GELU非线性激活函数,GELU是Gaussian Error Linear Units的缩写。
> - GELU比ReLU具有更强的表达能力。
> - 第二层仿射变换后输出。
> - 两个仿射变换层之间插入非线性激活GELU,增加了MLP模块的非线性表达能力。
> - 这个MLP结构通常堆叠在MSA模块之后,为Transformer编码器提供额外的非线性映射能力。

![CleanShot 2023-09-13 at 10.51.41@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1694573512486-634235cb-a108-4402-b8f7-624c43dfd4c5.png#averageHue=%23f5f5f5&clientId=u93340074-d581-4&from=paste&height=117&id=u316a0b9f&originHeight=234&originWidth=1310&originalType=binary&ratio=2&rotation=0&showTitle=false&size=43298&status=done&style=none&taskId=u1f01884c-c481-4bc4-ad8e-329a2994d78&title=&width=655)
**归纳偏置**。我们注意到,视觉Transformer比CNN具有更少的与图片相关的归纳偏置。在CNN中，局部性、二维邻域结构和平移等变性是贯穿整个模型的每一层固有的。在ViT中，只有MLP层是局部的和平移等变的，而自注意力层是全局的。二维邻域结构非常稀疏地使用：在模型开始处通过将图像切割成patch，以及在微调时调整不同分辨率图像的位置嵌入(如下所述)。除此之外，初始化时的位置嵌入不包含有关patch的2D位置的任何信息，所有patch之间的空间关系必须从零开始学习。
**融合架构**。作为原始图像patch的替代,输入序列可以由CNN(LeCun等,1989)的特征图「特征提取feature map」形成。在这种混合模型中，patch嵌入投影E(公式1)应用于从CNN特征图中提取的patch。作为一种特殊情况，patch可以具有1x1的空间大小，这意味着输入序列是通过简单展平特征图的空间维度并投影到Transformer维度而获得的。分类输入嵌入和位置嵌入的添加与上述相同。
**微调与更高的分辨率**
通常，我们在大型数据集上对ViT进行预训练，然后微调到(更小的)下游任务。为此，我们移除预训练的预测头，并添加一个零初始化的$D\times{K}$前馈层，其中$K$是下游类的数量。通常将分辨率提高到高于预训练时的分辨率进行微调是有益的(Touvron等,2019; Kolesnikov等,2020)。在输入更高分辨率的图像时，我们保持patch大小不变，这会导致更大的有效序列长度。Vision Transformer可以处理任意序列长度(受限于内存约束)，但是预训练的位置嵌入可能不再有意义。因此，我们根据位置嵌入在原始图像中的位置进行2D插值来调整预训练的位置嵌入。注意，分辨率调整和patch提取是手动向Vision Transformer注入图像2D结构的归纳偏置的唯二处。
### 实验
花分类的数据集，采用预训练的模式。
学习率：
![CleanShot 2023-09-14 at 23.31.12@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1694705517020-57f58f97-e0e5-4736-a00d-00ab0479b142.png#averageHue=%23fdfdfd&clientId=u5f6d3c89-0557-4&from=paste&height=476&id=u7a22a5c1&originHeight=952&originWidth=1774&originalType=binary&ratio=2&rotation=0&showTitle=false&size=99196&status=done&style=none&taskId=u71c4089e-e81e-4f3a-adef-b7141257e82&title=&width=887)
训练准确率：
![CleanShot 2023-09-14 at 23.32.58@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1694705600280-794d94b2-2a17-43a2-9e62-533cbb193319.png#averageHue=%23fdfdfd&clientId=u5f6d3c89-0557-4&from=paste&height=463&id=u222d80b2&originHeight=926&originWidth=1750&originalType=binary&ratio=2&rotation=0&showTitle=false&size=111557&status=done&style=none&taskId=uee516329-c20f-4029-afc0-c45359a1829&title=&width=875)
训练损失：
![CleanShot 2023-09-14 at 23.33.46@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1694705638835-ad8ee71c-c13c-42d2-bd89-c36db99b9424.png#averageHue=%23fcfcfc&clientId=u5f6d3c89-0557-4&from=paste&height=469&id=u52722372&originHeight=938&originWidth=1378&originalType=binary&ratio=2&rotation=0&showTitle=false&size=94982&status=done&style=none&taskId=ua54b5c32-5acf-434c-bbf8-f44ce6cc02e&title=&width=689)
验证集准确率：
![CleanShot 2023-09-14 at 23.34.23@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1694705686631-91ff7e66-669c-4c2b-88b7-a71b6309dc26.png#averageHue=%23fcfcfc&clientId=u5f6d3c89-0557-4&from=paste&height=474&id=u9143acef&originHeight=948&originWidth=1380&originalType=binary&ratio=2&rotation=0&showTitle=false&size=91299&status=done&style=none&taskId=u83f1e022-7637-4e75-91cb-58710cd0845&title=&width=690)
验证集损失：
![CleanShot 2023-09-14 at 23.35.01@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1694705718937-2a7efb51-8cb9-4e0d-bf52-461399185cfc.png#averageHue=%23fcfcfc&clientId=u5f6d3c89-0557-4&from=paste&height=475&id=ub7129030&originHeight=950&originWidth=1372&originalType=binary&ratio=2&rotation=0&showTitle=false&size=89136&status=done&style=none&taskId=u6d0451d1-1499-4dcb-a5d8-7978de21815&title=&width=686)
### 评论
### 代码实现
Vit-Base model (Vit-B/16), ImageNet-21k weights @ 224x224
```python
def vit_base_patch16_224_in21k(num_classes: int = 21843, has_logits: bool = True):
    """
    ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).
    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.
    weights ported from official Google JAX impl:
    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth
    """
    model = VisionTransformer(img_size=224,
                              patch_size=16,
                              embed_dim=768,
                              depth=12,
                              num_heads=12,
                              representation_size=768 if has_logits else None,
                              num_classes=num_classes)
    return model
```
```python
class VisionTransformer(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000,
                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True,
                 qk_scale=None, representation_size=None, distilled=False, drop_ratio=0.,
                 attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PatchEmbed, norm_layer=None,
                 act_layer=None):
        """
        Args:
            img_size (int, tuple): input image size
            patch_size (int, tuple): patch size
            in_c (int): number of input channels
            num_classes (int): number of classes for classification head
            embed_dim (int): embedding dimension
            depth (int): depth of transformer
            num_heads (int): number of attention heads
            mlp_ratio (int): ratio of mlp hidden dim to embedding dim
            qkv_bias (bool): enable bias for qkv if True
            qk_scale (float): override default qk scale of head_dim ** -0.5 if set
            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set
            distilled (bool): model includes a distillation token and head as in DeiT models
            drop_ratio (float): dropout rate
            attn_drop_ratio (float): attention dropout rate
            drop_path_ratio (float): stochastic depth rate
            embed_layer (nn.Module): patch embedding layer
            norm_layer: (nn.Module): normalization layer
        """
        super(VisionTransformer, self).__init__()
        self.num_classes = num_classes
        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
        self.num_tokens = 2 if distilled else 1
        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
        act_layer = act_layer or nn.GELU

        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))
        self.pos_drop = nn.Dropout(p=drop_ratio)

        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # stochastic depth decay rule
        self.blocks = nn.Sequential(*[
            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],
                  norm_layer=norm_layer, act_layer=act_layer)
            for i in range(depth)
        ])
        self.norm = norm_layer(embed_dim)

        # Representation layer
        if representation_size and not distilled:
            self.has_logits = True
            self.num_features = representation_size
            self.pre_logits = nn.Sequential(OrderedDict([
                ("fc", nn.Linear(embed_dim, representation_size)),
                ("act", nn.Tanh())
            ]))
        else:
            self.has_logits = False
            self.pre_logits = nn.Identity()

        # Classifier head(s)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
        self.head_dist = None
        if distilled:
            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()

        # Weight init
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        if self.dist_token is not None:
            nn.init.trunc_normal_(self.dist_token, std=0.02)

        nn.init.trunc_normal_(self.cls_token, std=0.02)
        self.apply(_init_vit_weights)

    def forward_features(self, x):
        # [B, C, H, W] -> [B, num_patches, embed_dim]
        x = self.patch_embed(x)  # [B, 196, 768]
        # [1, 1, 768] -> [B, 1, 768]
        cls_token = self.cls_token.expand(x.shape[0], -1, -1)
        if self.dist_token is None:
            x = torch.cat((cls_token, x), dim=1)  # [B, 197, 768]
        else:
            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)

        x = self.pos_drop(x + self.pos_embed)
        x = self.blocks(x)
        x = self.norm(x)
        if self.dist_token is None:
            return self.pre_logits(x[:, 0])
        else:
            return x[:, 0], x[:, 1]

    def forward(self, x):
        x = self.forward_features(x)
        if self.head_dist is not None:
            x, x_dist = self.head(x[0]), self.head_dist(x[1])
            if self.training and not torch.jit.is_scripting():
                # during inference, return the average of both classifier predictions
                return x, x_dist
            else:
                return (x + x_dist) / 2
        else:
            x = self.head(x)
        return x
```
## Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
### 摘要
这篇论文提出了一个新的视觉Transformer,名为Swin Transformer,它可以作为计算机视觉的通用 backbone。将Transformer从语言域迁移到视觉域面临的挑战来自这两个域之间的差异,例如视觉实体尺度的大幅变化以及图像中的像素分辨率相对文本中的单词要高得多。**为了解决这些差异,我们提出了一种分层Transformer,其表示是通过移动窗口计算的。移动窗口方案通过将自注意力计算限制在不重叠的本地窗口内,从而带来更高的效率,同时也允许跨窗口连接。这种分层体系结构具有在各种尺度建模的灵活性,其计算复杂度随着图像大小的线性增长。**Swin Transformer的这些特质使其与广泛的视觉任务兼容,包括图像分类(ImageNet-1K上的87.3% top-1准确率)和密集预测任务,如目标检测(COCO test-dev上的58.7个框AP和51.1个遮挡AP)和语义分割(ADE20K val上的53.5个mIoU)。其性能大大超过了之前的最先进技术,在COCO上的框AP提高了+2.7,遮挡AP提高了+2.6,在ADE20K上的mIoU提高了+3.2,展示了Transformer基础模型作为视觉backbone的潜力。 **分层设计和移动窗口方法对全MLP体系结构也是有益的**。代码和模型公开可用,网址为[https://github.com/microsoft/Swin-Transformer。](https://github.com/microsoft/Swin-Transformer%E3%80%82)
### 导言
计算机视觉建模长期以来一直被卷积神经网络(CNNs)主导。 从AlexNet [39]及其在ImageNet图像分类挑战中的革命性表现开始,CNN架构已经发展为通过更大规模[30,76]、更广泛的连接[34]和更复杂的卷积形式[70,18,84]变得越来越强大。 随着CNN成为各种视觉任务的backbone网络,这些架构进步提高了性能,从而广泛提升了整个领域。
另一方面,自然语言处理(NLP)中的网络架构演变走上了不同的道路,其中现今流行的架构是Transformer [64]。Transformer是为序列建模和转换任务设计的,其显着特点是使用注意力机制来对数据中的长程依赖进行建模。它在语言域的巨大成功促使研究人员研究如何将其适配到计算机视觉,最近在某些任务上展示了非常有希望的结果,特别是图像分类[20]和联合视觉语言建模[47]。
在这篇论文中,我们试图扩展Transformer的适用性,使其可以作为计算机视觉的通用backbone,就像它在自然语言处理中那样,以及CNN在视觉中那样。**我们观察到,将其在语言域中的高性能转移到视觉域中的重大挑战可以通过这两个模态之间的差异来解释。**这些差异之一涉及**规模**。与作为语言Transformer中基本处理元素的词符号不同,视觉元素的规模可以有很大差异,对象检测等任务中关注了这个问题[42,53,54]。在现有的基于Transformer的模型[64,20]中,**标记都是固定规模的,这种属性不适合这些视觉应用。**另一个区别是**图像中的像素分辨率远高于文本段落中的单词**。存在许多视觉任务,如语义分割,需要在像素级进行密集预测,这对于高分辨率图像上的Transformer将是不可行的,因为其自注意力的计算复杂度与图像大小平方相关。为了克服这些问题,我们提出了一个通用的Transformer backbone,称为Swin Transformer,**它构建分层特征图,计算复杂度随图像大小线性增长**。如图1(a)所示,Swin Transformer通过从小尺寸的patch(用灰色轮廓)开始,并在更深的Transformer层中逐渐合并相邻的patch来构建分层表示。有了这些分层特征图,Swin Transformer模型可以方便地利用特征金字塔网络(FPN)[42]或U-Net[51]等高级技术进行密集预测。通过在非重叠窗口(用红色轮廓)内局部计算自注意力来实现线性计算复杂度,这些窗口划分图像。每个窗口中的patch数量是固定的,因此复杂度成为图像大小的线性函数。这些优点使Swin Transformer适合作为各种视觉任务的通用backbone,与之前只产生单分辨率特征图且计算复杂度平方的基于Transformer的架构[20]形成对比。
![图1。 (a) 提出的Swin Transformer通过在更深层合并图像patch(灰色显示)构建分层特征图,并且由于仅在每个局部窗口(红色显示)内计算自注意力,所以计算复杂度随输入图像大小线性增长。 因此,它可以作为图像分类和密集识别任务的通用backbone。 (b) 相比之下,以前的视觉Transformer [20] 产生单个低分辨率的特征图,并且由于全局计算自注意力,计算复杂度随输入图像大小平方增长。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1695281625544-c04a96a0-ab11-4625-b476-fb1c935536f0.png#averageHue=%23edeae7&clientId=u6446a45f-8db5-4&from=paste&height=428&id=u4c78956e&originHeight=856&originWidth=1382&originalType=binary&ratio=2&rotation=0&showTitle=true&size=756282&status=done&style=none&taskId=u0d9c9b23-1fcd-4abf-b169-401138165ec&title=%E5%9B%BE1%E3%80%82%20%28a%29%20%E6%8F%90%E5%87%BA%E7%9A%84Swin%20Transformer%E9%80%9A%E8%BF%87%E5%9C%A8%E6%9B%B4%E6%B7%B1%E5%B1%82%E5%90%88%E5%B9%B6%E5%9B%BE%E5%83%8Fpatch%28%E7%81%B0%E8%89%B2%E6%98%BE%E7%A4%BA%29%E6%9E%84%E5%BB%BA%E5%88%86%E5%B1%82%E7%89%B9%E5%BE%81%E5%9B%BE%2C%E5%B9%B6%E4%B8%94%E7%94%B1%E4%BA%8E%E4%BB%85%E5%9C%A8%E6%AF%8F%E4%B8%AA%E5%B1%80%E9%83%A8%E7%AA%97%E5%8F%A3%28%E7%BA%A2%E8%89%B2%E6%98%BE%E7%A4%BA%29%E5%86%85%E8%AE%A1%E7%AE%97%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%2C%E6%89%80%E4%BB%A5%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6%E9%9A%8F%E8%BE%93%E5%85%A5%E5%9B%BE%E5%83%8F%E5%A4%A7%E5%B0%8F%E7%BA%BF%E6%80%A7%E5%A2%9E%E9%95%BF%E3%80%82%20%E5%9B%A0%E6%AD%A4%2C%E5%AE%83%E5%8F%AF%E4%BB%A5%E4%BD%9C%E4%B8%BA%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%92%8C%E5%AF%86%E9%9B%86%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1%E7%9A%84%E9%80%9A%E7%94%A8backbone%E3%80%82%20%28b%29%20%E7%9B%B8%E6%AF%94%E4%B9%8B%E4%B8%8B%2C%E4%BB%A5%E5%89%8D%E7%9A%84%E8%A7%86%E8%A7%89Transformer%20%5B20%5D%20%E4%BA%A7%E7%94%9F%E5%8D%95%E4%B8%AA%E4%BD%8E%E5%88%86%E8%BE%A8%E7%8E%87%E7%9A%84%E7%89%B9%E5%BE%81%E5%9B%BE%2C%E5%B9%B6%E4%B8%94%E7%94%B1%E4%BA%8E%E5%85%A8%E5%B1%80%E8%AE%A1%E7%AE%97%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%2C%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6%E9%9A%8F%E8%BE%93%E5%85%A5%E5%9B%BE%E5%83%8F%E5%A4%A7%E5%B0%8F%E5%B9%B3%E6%96%B9%E5%A2%9E%E9%95%BF%E3%80%82&width=691 "图1。 (a) 提出的Swin Transformer通过在更深层合并图像patch(灰色显示)构建分层特征图,并且由于仅在每个局部窗口(红色显示)内计算自注意力,所以计算复杂度随输入图像大小线性增长。 因此,它可以作为图像分类和密集识别任务的通用backbone。 (b) 相比之下,以前的视觉Transformer [20] 产生单个低分辨率的特征图,并且由于全局计算自注意力,计算复杂度随输入图像大小平方增长。")
Swin Transformer的一个关键设计元素是其在连续的自注意力层之间窗口划分的平移,如图2所示。移动窗口桥接了前一层的窗口,在它们之间提供了连接,这显著增强了建模能力(见表4)。这种策略在实际延迟方面也很有效:一个窗口内的所有查询patch共享相同的键集,这在硬件上方便内存访问。相比之下,早期的基于滑动窗口的自注意力方法[33,50]由于不同的查询像素具有不同的键集,在通用硬件上延迟较低。我们的实验表明,所提出的移动窗口方法比滑动窗口方法延迟要低得多,且建模能力相似(见表5和表6)。移动窗口方法对全MLP架构[61]也有益。
![图2展示了在提出的 Swin Transformer架构中计算自注意力的移动窗口方法。在第l层(左侧),采用常规的窗口划分方案,并在每个窗口内计算自注意力。在下一层l+1(右侧),窗口划分发生了移动,形成新的窗口。新窗口中的自注意力计算跨越了第l层中之前窗口的边界,在它们之间建立连接。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1695281962698-1956f8b0-0983-4b8b-a32f-9c2a556b15bd.png#averageHue=%23e5dfdb&clientId=u6446a45f-8db5-4&from=paste&height=250&id=u0e43227c&originHeight=500&originWidth=1266&originalType=binary&ratio=2&rotation=0&showTitle=true&size=528786&status=done&style=none&taskId=u65f5c0c5-4ee7-472a-8a1a-de26335fb0d&title=%E5%9B%BE2%E5%B1%95%E7%A4%BA%E4%BA%86%E5%9C%A8%E6%8F%90%E5%87%BA%E7%9A%84%20Swin%20Transformer%E6%9E%B6%E6%9E%84%E4%B8%AD%E8%AE%A1%E7%AE%97%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E7%A7%BB%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%96%B9%E6%B3%95%E3%80%82%E5%9C%A8%E7%AC%ACl%E5%B1%82%28%E5%B7%A6%E4%BE%A7%29%2C%E9%87%87%E7%94%A8%E5%B8%B8%E8%A7%84%E7%9A%84%E7%AA%97%E5%8F%A3%E5%88%92%E5%88%86%E6%96%B9%E6%A1%88%2C%E5%B9%B6%E5%9C%A8%E6%AF%8F%E4%B8%AA%E7%AA%97%E5%8F%A3%E5%86%85%E8%AE%A1%E7%AE%97%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E3%80%82%E5%9C%A8%E4%B8%8B%E4%B8%80%E5%B1%82l%2B1%28%E5%8F%B3%E4%BE%A7%29%2C%E7%AA%97%E5%8F%A3%E5%88%92%E5%88%86%E5%8F%91%E7%94%9F%E4%BA%86%E7%A7%BB%E5%8A%A8%2C%E5%BD%A2%E6%88%90%E6%96%B0%E7%9A%84%E7%AA%97%E5%8F%A3%E3%80%82%E6%96%B0%E7%AA%97%E5%8F%A3%E4%B8%AD%E7%9A%84%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%AE%A1%E7%AE%97%E8%B7%A8%E8%B6%8A%E4%BA%86%E7%AC%ACl%E5%B1%82%E4%B8%AD%E4%B9%8B%E5%89%8D%E7%AA%97%E5%8F%A3%E7%9A%84%E8%BE%B9%E7%95%8C%2C%E5%9C%A8%E5%AE%83%E4%BB%AC%E4%B9%8B%E9%97%B4%E5%BB%BA%E7%AB%8B%E8%BF%9E%E6%8E%A5%E3%80%82&width=633 "图2展示了在提出的 Swin Transformer架构中计算自注意力的移动窗口方法。在第l层(左侧),采用常规的窗口划分方案,并在每个窗口内计算自注意力。在下一层l+1(右侧),窗口划分发生了移动,形成新的窗口。新窗口中的自注意力计算跨越了第l层中之前窗口的边界,在它们之间建立连接。")
提出的Swin Transformer在图像分类、目标检测和语义分割识别任务上都取得了强劲的性能。它在三个任务上获得了类似的延迟,但显著优于ViT / DeiT [20,63]和ResNe(X)t模型[30,70]。它在COCO测试集上获得了58.7的框AP和51.1的遮挡AP,比之前的最先进成果分别提高了+2.7框AP(无外部数据的Copy-paste [26])和+2.6遮挡AP(DetectoRS [46])。在ADE20K语义分割上,它在验证集上获得了53.5的mIoU,比之前的最先进成果(SETR [81])提高了+3.2 mIoU。它在ImageNet-1K图像分类上也获得了87.3%的top-1准确率。
我们认为计算机视觉和自然语言处理的统一架构将有利于两个领域的发展,因为它将有助于视觉和文本信号的联合建模,两个领域的建模知识也可以更深入地共享。我们希望Swin Transformer在各种视觉问题上的强大表现可以加深社区对这一信念的理解,并鼓励视觉和语言信号的统一建模。
### 结论
本文提出了Swin Transformer,这是一个新的视觉Transformer,它产生分层特征表示,计算复杂度随输入图像大小线性增长。Swin Transformer在COCO目标检测和ADE20K语义分割上达到了最先进的性能,明显超过之前的最佳方法。我们希望Swin Transformer在各种视觉问题上的强大表现可以鼓励视觉和语言信号的统一建模。
作为Swin Transformer的一个关键元素,基于移动窗口的自注意力机制在视觉问题上被证明是有效和高效的,我们期待在自然语言处理中进一步调查它的使用。
### 相关工作
### 模型
#### 整体架构
Swin Transformer架构的概览如图3所示,展示了tiny版本(SwinT)。它首先通过patch分割模块(如ViT)将RGB图像分割成非重叠的patch。每个patch被视为一个“标记”,其特征被设置为原始像素RGB值的连接。在我们的实现中,我们使用4×4的patch大小,因此每个patch的特征维度为4×4×3=48。在这些原始值特征上应用线性嵌入层,将其投影到任意维度(表示为C)。
在这些patch标记上应用了几个具有修改后的自注意力计算的Transformer块(Swin Transformer块)。Transformer块保持标记的数量(H/4×W/4),与线性嵌入一起称为“第1阶段”。
为了产生分层表示,随着网络加深,patch合并层减少标记的数量。第一个patch合并层连接每个2×2相邻patch组的特征,并在4C维连接特征上应用线性层。这将标记的数量减少了2×2=4倍(分辨率减半2倍),输出维度设置为2C。之后应用Swin Transformer块进行特征变换,分辨率保持在H/8×W/8。这个首个patch合并和特征变换块被称为“第2阶段”。该过程重复两次,作为“第3阶段”和“第4阶段”,输出分辨率分别为H/16×W/16和H/32×W/32。这些阶段共同产生分层表示,具有与典型卷积网络(如VGG [52]和ResNet [30])相同的特征图分辨率。因此,所提出的架构可以方便地替换各种视觉任务中现有方法的backbone网络。
![图3 (a) Swin Transformer(Swin-T)的架构;(b) 两个连续的Swin Transformer块(用公式(3)表示)。W-MSA和SW-MSA分别是具有常规和移动窗口配置的多头自注意力模块。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1695283219271-208bf8c6-8c5b-4146-8c31-273b603072a2.png#averageHue=%23efefef&clientId=u6446a45f-8db5-4&from=paste&height=421&id=u15af8f2a&originHeight=842&originWidth=2632&originalType=binary&ratio=2&rotation=0&showTitle=true&size=257498&status=done&style=none&taskId=u782cdf77-0a2d-4c86-afa7-d1413040342&title=%E5%9B%BE3%20%28a%29%20Swin%20Transformer%28Swin-T%29%E7%9A%84%E6%9E%B6%E6%9E%84%3B%28b%29%20%E4%B8%A4%E4%B8%AA%E8%BF%9E%E7%BB%AD%E7%9A%84Swin%20Transformer%E5%9D%97%28%E7%94%A8%E5%85%AC%E5%BC%8F%283%29%E8%A1%A8%E7%A4%BA%29%E3%80%82W-MSA%E5%92%8CSW-MSA%E5%88%86%E5%88%AB%E6%98%AF%E5%85%B7%E6%9C%89%E5%B8%B8%E8%A7%84%E5%92%8C%E7%A7%BB%E5%8A%A8%E7%AA%97%E5%8F%A3%E9%85%8D%E7%BD%AE%E7%9A%84%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9D%97%E3%80%82&width=1316 "图3 (a) Swin Transformer(Swin-T)的架构;(b) 两个连续的Swin Transformer块(用公式(3)表示)。W-MSA和SW-MSA分别是具有常规和移动窗口配置的多头自注意力模块。")
**Swin Transformer block**
Swin Transformer是通过用基于移动窗口的模块(在3.2节中描述)替换Transformer块中的标准多头自注意力(MSA)模块构建的,其他层保持不变。如图3(b)所示,Swin Transformer块由基于移动窗口的MSA模块组成,后接一个2层MLP,中间是GELU非线性。在每个MSA模块和每个MLP之前应用LayerNorm(LN)层,并在每个模块之后应用残差连接。
#### 基于自注意力的移动窗口
标准的Transformer架构[64]及其用于图像分类的适配[20]都进行全局自注意,其中计算标记与所有其他标记之间的关系。全局计算导致计算复杂度随标记数量的平方增长,这使其不适合需要大量标记进行密集预测或表示高分辨率图像的许多视觉问题。
**Self-attention in non-overlapped windows**
为了高效建模,我们提出在局部窗口内计算自注意力。窗口的排布方式是以非重叠的方式均匀划分图像。假设每个窗口包含M×M个patch,则在包含h×w个patch的图像上,全局MSA模块和基于窗口的模块的计算复杂度为:
![CleanShot 2023-09-21 at 16.24.19@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1695284670157-95c5787d-11fb-4e0d-87d3-9082759f4dd5.png#averageHue=%23ededed&clientId=u6446a45f-8db5-4&from=paste&height=68&id=u01c9dcc5&originHeight=142&originWidth=830&originalType=binary&ratio=2&rotation=0&showTitle=false&size=28213&status=done&style=none&taskId=u8fb74e2d-c848-4f1b-b1b9-e53aa83f675&title=&width=396)
其中前者与patch数hw平方相关,后者在M固定时(默认设置为7)为线性关系。当hw很大时,通常无法承受全局自注意力的计算,而基于窗口的自注意力则具有可扩展性。
**Shifted window partitioning in successive blocks**
基于窗口的自注意力模块缺乏跨窗口连接,这限制了其建模能力。为了在保持非重叠窗口高效计算的同时引入跨窗口连接,我们提出了一个移动窗口划分方法,该方法在连续的Swin Transformer块之间交替使用两种划分配置。
如图2所示,第一个模块使用从左上角像素开始的常规窗口划分策略,8×8特征图被均匀划分为4×4大小的2×2窗口(M=4)。然后,下一模块采用从前一层移动$(\lfloor\frac{M}{2}\rfloor,\lfloor\frac{M}{2}\rfloor)$像素的窗口配置,从经常划分的窗口中移位。通过移动窗口划分方法,连续的Swin Transformer块计算如下:
![CleanShot 2023-09-21 at 16.32.50@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1695285177956-dbde757d-1c27-47c7-ab22-942fd8cfc8b1.png#averageHue=%23f0f0f0&clientId=u6446a45f-8db5-4&from=paste&height=104&id=u2a649fc2&originHeight=284&originWidth=878&originalType=binary&ratio=2&rotation=0&showTitle=false&size=45012&status=done&style=none&taskId=ufc1d70e1-a577-4f90-933c-e5264860edf&title=&width=323)
其中,W-MSA和SW-MSA分别表示采用常规和移动窗口配置的多头自注意力模块。这种简单的移动窗口方法可以有效建立跨窗口连接,同时保持计算效率。其中,W-MSA和SW-MSA分别表示采用常规和移动窗口配置的多头自注意力模块。
**Efficient batch computation for shifted configuration**
滑动窗口分区的一个问题是，在移位配置中将会产生更多的窗口，从$\lceil\frac{h}{M}\rceil\times\lceil\frac{w}{M}\rceil$到$([\frac{h}{M}]+1)\times([\frac{w}{M}]+1)$，而其中一些窗口会小于M × M 。一个简单的解决方案是将较小的窗口填充到大小为M × M，并在计算注意力时屏蔽填充的值。当常规分区中窗口的数量较少时，例如2 × 2，使用这种简单方法增加了相当多的计算量（2 × 2 → 3 × 3，增加了2.25倍）。在这里，我们提出了一种更高效的批处理计算方法，通过向左上方周期性移动，如图4所示。在这种移动之后，一个批处理窗口可能由多个在特征图中不相邻的子窗口组成，因此采用了一个屏蔽机制来限制自注意计算仅在每个子窗口内进行。通过循环移位，批处理窗口的数量保持与常规窗口分区相同，因此也非常高效。这种方法的低延迟在表5中有所展示。
**Relative position bias**
在计算自注意力时，我们按照[49, 1, 32, 33]的方法，对于每个注意力头部，都在计算相似性时添加了一个相对位置偏置$B\in \mathbb{R}^{M^{2}\times M^{2}}$。
![CleanShot 2023-09-21 at 17.11.27@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1695287495280-841fb032-ac05-4411-a1c6-f02b51cf2b7f.png#averageHue=%23eeeeee&clientId=u6446a45f-8db5-4&from=paste&height=39&id=ue36538ca&originHeight=94&originWidth=992&originalType=binary&ratio=2&rotation=0&showTitle=false&size=19954&status=done&style=none&taskId=ue30e8633-6219-47d9-a7f1-4f66aa51501&title=&width=411)
其中，$Q,K,V \in \mathbb{R}^{{M^2}\times{d}}$分别表示查询、键和值矩阵；d 是查询/键的维度，而 $M^2$ 是一个窗口中的patch数。由于每个轴上的相对位置位于范围[-M + 1，M - 1]内，我们将一个较小尺寸的偏置矩阵$\hat{B}\in{{\mathbb{R}^{(2M-1)\times(2M-1)}}}$进行参数化，而B中的值则来自于$\hat{B}$。
我们观察到相对位置偏置项的显著改进，与没有此偏置项或使用绝对位置嵌入的对照方法相比，如表4所示。进一步将绝对位置嵌入添加到输入中，就像[20]中所示，会略微降低性能，因此我们的实现中没有采用这种方法。
在预训练中学到的相对位置偏置可以用于初始化一个模型，以便通过双三次插值[20, 63]适用于不同窗口大小的微调。
### 实验
### 评论
### 代码实现
## Meta-Transformer: A Unified Framework for Multimodal Learning
### 摘要
多模态学习旨在构建能够处理和关联来自多个模态的信息的模型。尽管在这一领域已经有多年的发展，但由于它们之间的内在差距，设计一个用于处理各种模态（例如自然语言、2D图像、3D点云、音频、视频、时间序列、表格数据等）的统一网络仍然具有挑战性。在这项工作中，**我们提出了一个名为“Meta-Transformer”的框架，利用一个冻结的编码器来执行多模态感知，而无需任何成对的多模态训练数据。在Meta-Transformer中，来自各种模态的原始输入数据被映射到共享的标记空间，从而使随后具有冻结参数的编码器能够提取输入数据的高级语义特征。Meta-Transformer由三个主要组件组成：统一的数据标记器、模态共享的编码器以及用于下游任务的任务特定头部。**Meta-Transformer是第一个可以在没有成对数据的情况下跨足12个模态进行统一学习的框架。在不同的基准测试上的实验结果表明，Meta-Transformer可以处理广泛的任务，包括基本感知（文本、图像、点云、音频、视频）、实际应用（X射线、红外、高光谱和IMU）、以及数据挖掘（图形、表格和时间序列）。Meta-Transformer为使用Transformer开发统一多模态智能指明了一个有前途的未来。代码在下面的网址中。
[GitHub - invictus717/MetaTransformer: Meta-Transformer for Unified Multimodal Learning](https://github.com/invictus717/MetaTransformer)
### 导言
人脑被认为是神经网络模型的灵感来源，它同时处理来自各种感官输入（如视觉、听觉和触觉信号）的信息。此外，从一个信息源获取的知识可以有益于理解另一个信息源。然而，在深度学习中，由于显著的模态差距，设计一个能够处理各种数据格式的统一网络是一项非常复杂的任务[1-3]。
每种数据模态呈现出独特的数据模式，这使得难以将在一个模态上训练的模型适应到另一个模态上。例如，图像由于像素密集，存在高度信息冗余，而自然语言则不然[4]。另一方面，点云在3D空间中分布稀疏，更容易受到噪声干扰，难以表示[5]。音频频谱图是随时间变化且非静态的数据模式，由频域中波形的组合构成[6]。视频数据包含一系列图像帧，具有捕捉空间信息和时间动态的独特能力[7]。图形数据将实体表示为图中的节点，将关系表示为边，模拟实体之间复杂的多对多关系[8]。**由于不同数据模态之间的显著差异，通常使用不同的网络架构来分别编码每个模态。**例如，Point Transformer[9] 利用矢量级别的位置注意力从3D坐标中提取结构信息，但不能编码图像、自然语言段落或音频频谱图切片。因此，设计一个能够利用共享模态参数空间来编码多个数据模态的统一框架仍然是一个重大挑战。最近，像VLMO[2]、OFA[10]和BEiT-3[3]这样的统一框架的发展通过大规模多模态预训练（3,10,2）改善了网络对多模态理解的能力，但它们更侧重于视觉和语言，并不能跨模态共享整个编码器。2017年Vaswani等人提出的Transformer架构和注意机制在自然语言处理（NLP）中取得了重大进展[11-16]。这些进展在增强了不同模态的感知方面起到了关键作用，例如2D视觉（包括ViT[17,18]和Swin Transformer[19]）、3D视觉（如Point Transformer[9]和Point-ViT[20,21]）以及音频信号处理（AST[6]）等。这些研究展示了基于Transformer的架构的多功能性，激发了研究人员的兴趣，探索是否可能开发能够统一多个模态的基础模型，最终实现在所有模态上的人类水平感知。
![Table 1: Comparison between Meta-Transformer and related works on perception tasks.](https://cdn.nlark.com/yuque/0/2023/png/25721528/1695818533850-fd1374bf-7908-4763-972b-7976d7130f24.png#averageHue=%23e6e6e6&clientId=u975544f7-aa17-4&from=paste&height=133&id=u24a0f499&originHeight=266&originWidth=1146&originalType=binary&ratio=2&rotation=0&showTitle=true&size=85638&status=done&style=none&taskId=u9a325a3b-f8dc-428d-8bf1-9378671d20a&title=Table%201%3A%20Comparison%20between%20Meta-Transformer%20and%20related%20works%20on%20perception%20tasks.&width=573 "Table 1: Comparison between Meta-Transformer and related works on perception tasks.")
我们在包括12个不同模态的各种基准测试上进行了广泛的实验。通过仅使用LAION-2B [24]数据集的图像进行预训练，Meta-Transformer在处理来自多个模态的数据方面表现出卓越的性能，在不同的多模态学习任务中持续取得优于最先进方法的结果。更详细的实验设置可以在第D节中找到。
总之，我们的贡献可以总结如下：

- 在多模态研究领域，我们提出了一个新颖的框架，Meta-Transformer，它能够使用相同的参数集同时从多个模态中提取表示。
- 对于多模态网络设计，我们全面研究了Transformer组件（如嵌入、标记化和编码器）在处理各种模态数据时的功能。Meta-Transformer提供了有价值的见解，并激发了在开发一个能够统一所有模态的模态无关框架方面的有前途的新方向。
- 在实验方面，Meta-Transformer在关于12个模态的各种数据集上实现了出色的性能，这验证了Meta-Transformer在统一多模态学习方面的进一步潜力。
### 结论
总之，我们认为Meta-Transformer实现多模态学习的能力源于神经网络架构可以学习模态不变的模式。Meta-Transformer的架构展示了在多模态学习中长度可变的标记嵌入的优势，提供了灵活但统一的多模态语义形式。现在是思考如何设计算法来训练网络以在未见过的模态上进行泛化的时候了。与此同时，设计一个统一的多模态解码器的架构，可以将表示解码成特定模态的任何形式，也具有很大的吸引力。尽管Meta-Transformer展现出了令人惊讶的性能，并在多模态感知方面展示了一个新的有前途的方向，但我们不确定所提出的架构是否在生成任务中同样有效。如何开发模态不变的生成模型仍然是一个谜。我们希望这可以激发未来的研究。
### 相关工作
#### Single-Modality Perception（单模态感知）
各种神经网络的发展促进了机器智能的感知[27-29, 11]。
**Multi-Layer Perceptron for pattern recognition（多层感知器用于模式识别）**：最初，支持向量机（SVM）和多层感知器（MLP）被应用于文本[30]、图像[31]、点云[32]和音频[33]的分类。这些创新工作证明了引入AI到模式识别的可行性。
**Recurrent & Convolutional Neural Network（循环神经网络和卷积神经网络）：**Hopfield网络[34]是循环网络的最初形式，然后LSTM[35]和GRU[36]进一步探索了RNN在序列建模和NLP任务应用中的优势，也广泛应用于音频合成[40]。与此同时，包括LeNet[41]、AlexNet[42]、VGG[43]、GoogleNet[44]和ResNet[29]在图像识别中的成功极大推动了CNN在其他领域的应用，如文本分类[45,46]、点云理解[47-49]和语音分类[50]。
**Transformer**：最近，transformer架构[11]已在各种任务中得到应用，包括NLP中的文本理解[51]和生成[52]，图像中的分类[13]、检测[53]和分割[15]，点云理解[22,9]以及音频识别[6,23]。
然而，类似于CNN和RNN的应用，这些网络根据不同模态的特性进行了修改。目前还没有用于模态无关学习的通用架构。更重要的是，来自不同模态的信息可以相互补充[54-56]，因此设计一个能够编码来自不同模态的数据并通过共享参数空间连接这些复杂表示的框架具有重要意义。
#### Transformed-based Multimodal Perception（基于Transformer的多模态感知）
Transformer模型在感知方面的优势包括全局感知域和相似性建模，这显著促进了多模态感知的发展。MCAN[57]提出了视觉和语言之间的深层模块化共注意网络，通过简洁地最大化交叉注意力来执行跨模态对齐。然后，通过利用交叉注意力机制来连接不同模态成为共识[2,1,10,3]。随着预训练微调范式的成功，越来越多的研究关注如何通过预训练有效地对齐跨模态提取的表示。VL-BERT[58]首创了使用MLM范式为通用视觉语言理解提供模态对齐的表示。然后Oscar[59]描述了视觉和文本内容中的对象语义。诸如Vinvl[60]、Simvlm[1]、VLMO[2]、ALBEF[61]和Florence[62]等框架进一步探讨了关于语义一致性方面跨视觉-语言模态的联合表示的优势。
多模态模型还用于少样本学习[25]、序列到序列学习[10]、对比学习[63]。BEiT-v3[3]提出将图像视为一种外语，并采用更精细的跨模态掩码和重构过程，共享部分参数。MoMo[64]在使用相同编码器处理图像和文本时进一步探讨了训练策略和目标函数。
尽管取得了这些进展，由于不同模态之间的差异，设计统一的多模态网络仍然存在显著障碍。此外，该领域大多数研究都侧重于视觉和语言任务，可能不会直接应对3D点云理解、音频识别或其他模态等挑战。Flamingo模型[25]代表了强大的少样本学习器，但其对点云的可迁移性有限，利用从一种模态中获取的先前知识以使其他模态受益仍然是一项挑战。换句话说，现有的多模态方法在更多模态上的可扩展性有限，尽管它们具有昂贵的训练成本。解决这些差异取决于使用相同一组参数来连接不同模态，类似于桥梁连接多个河岸。
### 模型
在本节中，我们详细描述了提出的框架Meta-Transformer。Meta-Transformer将来自不同模态的数据处理管道统一起来，使用共享的编码器来编码文本、图像、点云、音频和其他8种模态的数据。为了实现这一目标，Meta-Transformer由以下组成：数据到序列标记器，用于将数据投影到共享的嵌入空间；模态无关编码器，用于编码不同模态的嵌入；以及用于执行下游预测的任务特定头部，如图2所示。
![图2：Meta-Transformer包括数据到序列标记化、统一特征编码和下游任务学习。该框架以文本、图像、点云和音频为例进行了说明。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1695822634003-ecaef899-d0a4-4b47-b076-b0f7e5ddd5ad.png#averageHue=%23b79971&clientId=u975544f7-aa17-4&from=paste&height=402&id=u1f8fd404&originHeight=804&originWidth=1799&originalType=binary&ratio=2&rotation=0&showTitle=true&size=493663&status=done&style=none&taskId=u1f24aa16-aa6e-4dce-be13-f5dc9f9e7e5&title=%E5%9B%BE2%EF%BC%9AMeta-Transformer%E5%8C%85%E6%8B%AC%E6%95%B0%E6%8D%AE%E5%88%B0%E5%BA%8F%E5%88%97%E6%A0%87%E8%AE%B0%E5%8C%96%E3%80%81%E7%BB%9F%E4%B8%80%E7%89%B9%E5%BE%81%E7%BC%96%E7%A0%81%E5%92%8C%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E3%80%82%E8%AF%A5%E6%A1%86%E6%9E%B6%E4%BB%A5%E6%96%87%E6%9C%AC%E3%80%81%E5%9B%BE%E5%83%8F%E3%80%81%E7%82%B9%E4%BA%91%E5%92%8C%E9%9F%B3%E9%A2%91%E4%B8%BA%E4%BE%8B%E8%BF%9B%E8%A1%8C%E4%BA%86%E8%AF%B4%E6%98%8E%E3%80%82&width=899.5 "图2：Meta-Transformer包括数据到序列标记化、统一特征编码和下游任务学习。该框架以文本、图像、点云和音频为例进行了说明。")
#### Preliminary（初步的）
正式来说，我们将n个模态的输入空间表示为$\{\mathcal{X}_1,\mathcal{X}_2,\cdots,\mathcal{X}_n\}$，它们的对应标签空间为$\{\mathcal{Y}_1,\mathcal{Y}_2,\cdots,\mathcal{Y}_n\}$。此外，我们假设对于每个模态存在一个有效的参数空间$\Theta_{i}$，其中可以利用任何参数$\theta_i\in\Theta_i$来处理来自该模态的数据$\boldsymbol{x}_i\in\mathcal{X}_i$，我们说Meta-Transformer的本质是找到一个共享的$\boldsymbol{\theta}^{*}$，满足以下条件：
$\theta^*\in\Theta_1\cap\Theta_2\cap\Theta_3\cap\cdots\Theta_n,$(1)
并假设：
$\Theta_1\cap\Theta_2\cap\Theta_3\cap\cdots\Theta_n\neq\varnothing.$(2)
多模态神经网络可以被形式化为一个统一的映射函数$\mathcal{F}:\boldsymbol{x}\in\mathcal{X}\to \hat{y}\in\mathcal{Y}$其中$x$是来自任何模态$\{\mathcal{X}_1,\mathcal{X}_2,\cdots,\mathcal{X}_n\}$的输入数据，而$\hat{y}$表示网络的预测。我们用$y$表示真实标签，多模态处理流程可以被形式化为：
$\hat{y}=\mathcal{F}(\boldsymbol{x};\theta^*),\begin{array}{c}\theta^*=\arg\min_{x\in\mathcal{X}}[\mathcal{L}(\hat{y},y)].\end{array}$(3)
#### Data-to-Sequence Tokenization（数据到序列标记化）
我们提出了一种新颖的元标记化（meta-tokenization）方案，旨在将各种模态的数据转换为标记嵌入，全部在一个共享的流形空间（shared manifold space）中完成。然后，将这种方法应用于标记化（tokenization），考虑了模态的实际特征，如图3所示。我们以文本、图像、点云和音频为例。更多详细信息可以在补充资料中找到。具体来说，我们使用$\boldsymbol{x}_T,\boldsymbol{x}_I,\boldsymbol{x}_P,\operatorname{and}\boldsymbol{x}_A$来表示文本、图像、点云和音频频谱图的数据样本。
**Natural Language. **遵循通常的做法[51, 65]，我们使用带有30,000个标记词汇的WordPiece嵌入[66]。WordPiece将原始单词分割为子词。例如，原始句子："The supermarket is hosting a sale"，可能被WordPiece分割为："_The _super market _is _host ing _a _sale"。
在这种情况下，单词"supermarket"被分割成两个子词"_super"和"market"，而单词"hosting"被分割成"_host"和"ing"，而其余单词保持不变，仍然是单个单元。每个原始单词的开头会用特殊字符"_"叠加，表示自然单词的开始。每个子词对应于词汇表中的唯一标记，然后映射到具有词嵌入层的高维特征空间。结果，每个输入文本都被转换为一组标记嵌入$\boldsymbol{x}\in\mathbb{R}^{n\times D}$，其中$n$是标记的数量，$D$是嵌入的维度。
**Images**.为了适应2D图像，我们将图像$\boldsymbol{x}\in\mathbb{R}^{H\times W\times C}$重塑为扁平化的2D块序列$\boldsymbol{x}_p\in\mathbb{R}^{N_s\times(S^2\cdot C)}$，其中$(H,W)$表示原始图像分辨率，$C$表示通道数；$S$是块的大小，$N_s=(HW/S^2)$是生成的块数。之后，我们使用一个投影层将嵌入维度投影到$D$：
$\boldsymbol{x}_I\in\mathbb{R}^{C\times H\times W}\to\boldsymbol{x}_I^{\prime}\in\mathbb{R}^{N_s\times(S^2\cdot C)}\to\boldsymbol{x}_I^{\prime\prime}\in\mathbb{R}^{N_s\times D}.$
请注意，对于红外图像，我们使用相同的操作，但对于高光谱图像，我们使用线性投影。此外，对于视频识别，我们仅仅将2D卷积层替换为3D卷积。更多详细信息可以在B.1和B.3中找到。
**Point Cloud**. 为了使用Transformer学习3D模式，我们将点云从原始输入空间转换为标记嵌入空间。$\mathcal{X}=\{\boldsymbol{x}_i\}_{i=1}^P$表示包含$P$个点的点云，其中$\boldsymbol{x}_i=(\boldsymbol{p}_i,\boldsymbol{f}_i)$，$\boldsymbol{p}_i\in\mathbb{R}^3$表示3D坐标，$\boldsymbol{f}_i\in\mathbb{R}^c$是第$i$个点的特征。通常，$f_i$包含视觉提示，如颜色、视点、法线等。我们使用最远点采样（FPS）操作以固定的采样比例（1/4）来采样原始点云的代表性骨架。然后，我们使用K最近邻（KNN）来将邻近的点分组。基于包含局部几何先验的分组集，我们构建了包含分组子集的中心点的邻接矩阵，以进一步揭示3D对象和3D场景的综合结构信息。
![图3：数据到序列标记化的示意图3.2。我们提出了元方案，如（a）中所示，包含分组、卷积和变换过程。然后，（b）-（e）表示应用在文本、图像、点云和音频频谱上的构建块，使用我们的元方案。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1696045868236-b77a1be3-9a42-4939-9283-3ee1e188d472.png#averageHue=%23f7ddb4&clientId=u9b4f9906-e15a-4&from=paste&height=384&id=ubb069a66&originHeight=768&originWidth=1783&originalType=binary&ratio=2&rotation=0&showTitle=true&size=118892&status=done&style=none&taskId=ua75ce0e3-9ffc-4e66-a9cb-71a97c4d064&title=%E5%9B%BE3%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%88%B0%E5%BA%8F%E5%88%97%E6%A0%87%E8%AE%B0%E5%8C%96%E7%9A%84%E7%A4%BA%E6%84%8F%E5%9B%BE3.2%E3%80%82%E6%88%91%E4%BB%AC%E6%8F%90%E5%87%BA%E4%BA%86%E5%85%83%E6%96%B9%E6%A1%88%EF%BC%8C%E5%A6%82%EF%BC%88a%EF%BC%89%E4%B8%AD%E6%89%80%E7%A4%BA%EF%BC%8C%E5%8C%85%E5%90%AB%E5%88%86%E7%BB%84%E3%80%81%E5%8D%B7%E7%A7%AF%E5%92%8C%E5%8F%98%E6%8D%A2%E8%BF%87%E7%A8%8B%E3%80%82%E7%84%B6%E5%90%8E%EF%BC%8C%EF%BC%88b%EF%BC%89-%EF%BC%88e%EF%BC%89%E8%A1%A8%E7%A4%BA%E5%BA%94%E7%94%A8%E5%9C%A8%E6%96%87%E6%9C%AC%E3%80%81%E5%9B%BE%E5%83%8F%E3%80%81%E7%82%B9%E4%BA%91%E5%92%8C%E9%9F%B3%E9%A2%91%E9%A2%91%E8%B0%B1%E4%B8%8A%E7%9A%84%E6%9E%84%E5%BB%BA%E5%9D%97%EF%BC%8C%E4%BD%BF%E7%94%A8%E6%88%91%E4%BB%AC%E7%9A%84%E5%85%83%E6%96%B9%E6%A1%88%E3%80%82&width=891.5 "图3：数据到序列标记化的示意图3.2。我们提出了元方案，如（a）中所示，包含分组、卷积和变换过程。然后，（b）-（e）表示应用在文本、图像、点云和音频频谱上的构建块，使用我们的元方案。")
最后，我们聚合来自K个子集的结构表示。我们得到点嵌入如下：
$\boldsymbol{x}_P\in\mathbb{R}^{P\times(3+c)}\to\boldsymbol{x}_P^{\prime}\in\mathbb{R}^{\frac P4\times\frac D2}\to\boldsymbol{x}_P^{\prime\prime}\in\mathbb{R}^{\frac P{16}\times D}$
**Audio Spectrogram. **首先，我们使用长度为$t$秒的log Mel滤波器组对音频波形进行预处理[67]。然后，我们在频率上以$f_s$的频率步长$t_s$使用(Hamming window)汉明窗口将原始波形分成$l = (t/t_s)$个间隔，并进一步将原始波形转换为$l$维的滤波器组。
接着，我们从时间和频率维度使用相同大小的$S$来将频谱图分割成块。与图像块不同，音频块在频谱图上有重叠。遵循AST [6]，我们还选择通过$S \times S$卷积将整个频谱图分割成$N_s = 12[(100t − 16)/10]$个块，然后将这些块扁平化成标记序列。最后，总结该过程：
$\boldsymbol{x}_A\in\mathbb{R}^{T\times F}\to\boldsymbol{x}_A^{\prime}\in\mathbb{R}^{N_s\times S\times S}\to\boldsymbol{x}_A^{\prime\prime}\in\mathbb{R}^{(N_s\cdot D/S^2)\times D}$
这里，$T$和$F$表示时间和频率维度。
#### Unified Encoder
在将原始输入转换为标记嵌入空间后，我们利用具有冻结参数的统一Transformer编码器来编码来自不同模态的标记嵌入序列。预训练阶段，我们使用ViT [13]作为骨干网络，并在LAION-2B数据集上进行对比学习的预训练，以增强通用标记编码的能力。预训练后，我们冻结骨干网络的参数。此外，对于文本理解，我们使用CLIP [24]的预训练文本分词器来将句子分割成子词，并将子词转换成单词嵌入。
模态无关学习。按照通常的做法[51, 13]，我们在标记嵌入序列前添加一个可学习的$x_{CLS}$标记，$x_{CLS}$标记的最终隐藏状态$(\boldsymbol{z}_L^0)$用作输入序列的摘要表示，通常用于执行识别任务。为了强调位置信息，我们将位置嵌入合并到标记嵌入中。需要注意的是，我们将输入数据标记为1D嵌入，因此我们选择标准的可学习1D位置嵌入。此外，我们在图像识别中未观察到更复杂的2D位置嵌入能够显著提高性能。我们通过元素逐个相加操作将位置嵌入和内容嵌入简单地融合在一起，然后将得到的嵌入序列输入编码器。
具有深度L的Transformer编码器包括多个堆叠的多头自注意（MSA）层和MLP块。首先，将输入的标记嵌入传递到MSA层，然后是MLP块。然后，$(\ell-1)$层MLP块的输出作为$\ell$层MSA层的输入。在每一层之前附加层归一化（LN），并在每一层之后应用残差连接。MLP包含两个线性全连接层和一个GELU非线性激活。Transformer的公式如下：
![CleanShot 2023-09-30 at 13.01.26@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1696050095615-09a46426-80da-4439-9547-5a5da5cac585.png#averageHue=%23f5f5f5&clientId=u9b4f9906-e15a-4&from=paste&height=86&id=u734eb333&originHeight=282&originWidth=1638&originalType=binary&ratio=2&rotation=0&showTitle=false&size=62714&status=done&style=none&taskId=u0a767fd3-f80c-45cb-9aa4-4259210b8fe&title=&width=498)
这里，$E_x$表示来自提出的分词器的标记嵌入，$n$表示标记的数量。我们通过位置嵌入$E_{pos}$来增强块嵌入和可学习嵌入。
#### Task-Specific Heads
在获得学习表示之后，我们将这些表示馈送到任务特定的头部$h(\cdot;\theta_h)$，这些头部主要由多层感知器（MLP）组成，根据模态和任务的不同而变化。Meta-Transformer的学习目标可以总结为：
$\hat{\boldsymbol{y}}=\mathcal{F}(\boldsymbol{x};\theta^*)=h\circ g\circ f(\boldsymbol{x}),\quad\theta^*=\arg\min_\theta\mathcal{L}(\hat{y},y)$
其中，$f(\cdot)$,$g(\cdot)$和$h(\cdot)$分别表示分词器、骨干网络和头部的函数。
### 实验
### 评论
### 代码实现
## ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision
### 摘要
### 导言
### 结论
### 相关工作
### 模型
### 实验
### 评论
### 代码实现
## ViST: A Ubiquitous Model with Multimodal Fusion for Crop Growth Prediction
### 摘要
作物生长预测可以帮助农业工作者在农业活动中做出准确和合理的决策。现有的作物生长预测模型主要关注单一作物，并为每种作物训练单独的模型。在本文中，我们将开发一个适用于多种作物的普适生长预测模型，旨在为多种作物训练一个单一模型。我们开发了一种适用于作物生长预测的普适视觉和传感器变换器（ViST）模型，该模型利用图像和传感器数据。在所提出的模型中，提出了一种交叉注意力机制，以实现多模态特征图的融合，以降低计算成本并平衡特征之间的相互作用效果。为了训练该模型，我们将来自多种作物的数据结合起来，训练一个单一的（ViST）模型。在种植稻谷、大豆和玉米的农场上构建了一个传感器网络系统，用于数据收集。实验结果表明，所提出的ViST模型具有出色的适用性，可用于多种作物的生长预测。
### 导言
### 结论
本论文提出了一种基于Transformer的ViST模型，用于利用图像和传感器数据在农场进行作物生长预测。模型中的交叉注意力机制被用来改善模型数据融合的效果。三种作物的数据被一起训练作为模型的输入。该模型不仅实现了高准确性，而且保持了相对较快的速度。实验结果显示，多模态数据的模型可以提高作物生长预测的效果。尽管ViST模型取得了良好的结果，但仍存在一些限制。由于模型需要进行跨模态特征融合，这可能会增加模型的计算成本和训练时间。模型的广泛适用性需要在不同作物上进行进一步验证，以确定其适用性和通用性。在未来，将收集更多的作物生长数据以进行模型优化。此外，研究该模型在实际农业生产中的应用可以帮助农民更好地决策和生产管理。
### 相关工作
### 模型
所提出的ViST模型的整体网络结构如图1所示。输入包括作物图像和传感器数据。在这个框架中，MLP模块和线性投影Flattened Patches模块（LPFP）分别从传感器数据和图像数据中提取特征。LPFP是由ViT[55]引入的。变压器编码器用于数据融合。Pooler和Linear模块旨在减小特征的维度。框架中每个模块的具体细节如下所述。
![图1. ViST用于生长预测的框架。实线表示正向传递，虚线表示循环。ViST的输入是传感器和图像数据。它们分别在MLP和LPFP模块中独立处理。来自两个模块的特征被输入到一个Transformer编码器中进行特征融合。编码器的输出被提供给具有多模态输出的Concat模块（MM）。同时，这些特征被分别发送到其他两个Transformer编码器中进行自注意力机制。这两个Transformer编码器的输出分别是具有图像的单模态输出（SMI）和具有传感器的单模态输出（SMS）。MM、SMI和SMS的结果然后被输入到Pooler模块中，以减小特征的维度。最后，这些特征被输入到线性层模块，以输出叶面积指数（LAI）的值（在[0,1]范围内）。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697338290691-ecd40e35-f4a5-43de-8284-3a5ce190b93e.png#averageHue=%23e5ba84&clientId=u8f1acba0-7063-4&from=paste&height=703&id=KN8Er&originHeight=1406&originWidth=2716&originalType=binary&ratio=2&rotation=0&showTitle=true&size=610543&status=done&style=none&taskId=uc3a624c0-613c-44e3-ba38-ed2685fbe30&title=%E5%9B%BE1.%20ViST%E7%94%A8%E4%BA%8E%E7%94%9F%E9%95%BF%E9%A2%84%E6%B5%8B%E7%9A%84%E6%A1%86%E6%9E%B6%E3%80%82%E5%AE%9E%E7%BA%BF%E8%A1%A8%E7%A4%BA%E6%AD%A3%E5%90%91%E4%BC%A0%E9%80%92%EF%BC%8C%E8%99%9A%E7%BA%BF%E8%A1%A8%E7%A4%BA%E5%BE%AA%E7%8E%AF%E3%80%82ViST%E7%9A%84%E8%BE%93%E5%85%A5%E6%98%AF%E4%BC%A0%E6%84%9F%E5%99%A8%E5%92%8C%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E3%80%82%E5%AE%83%E4%BB%AC%E5%88%86%E5%88%AB%E5%9C%A8MLP%E5%92%8CLPFP%E6%A8%A1%E5%9D%97%E4%B8%AD%E7%8B%AC%E7%AB%8B%E5%A4%84%E7%90%86%E3%80%82%E6%9D%A5%E8%87%AA%E4%B8%A4%E4%B8%AA%E6%A8%A1%E5%9D%97%E7%9A%84%E7%89%B9%E5%BE%81%E8%A2%AB%E8%BE%93%E5%85%A5%E5%88%B0%E4%B8%80%E4%B8%AATransformer%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%AD%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%82%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E8%BE%93%E5%87%BA%E8%A2%AB%E6%8F%90%E4%BE%9B%E7%BB%99%E5%85%B7%E6%9C%89%E5%A4%9A%E6%A8%A1%E6%80%81%E8%BE%93%E5%87%BA%E7%9A%84Concat%E6%A8%A1%E5%9D%97%EF%BC%88MM%EF%BC%89%E3%80%82%E5%90%8C%E6%97%B6%EF%BC%8C%E8%BF%99%E4%BA%9B%E7%89%B9%E5%BE%81%E8%A2%AB%E5%88%86%E5%88%AB%E5%8F%91%E9%80%81%E5%88%B0%E5%85%B6%E4%BB%96%E4%B8%A4%E4%B8%AATransformer%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%AD%E8%BF%9B%E8%A1%8C%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E3%80%82%E8%BF%99%E4%B8%A4%E4%B8%AATransformer%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E8%BE%93%E5%87%BA%E5%88%86%E5%88%AB%E6%98%AF%E5%85%B7%E6%9C%89%E5%9B%BE%E5%83%8F%E7%9A%84%E5%8D%95%E6%A8%A1%E6%80%81%E8%BE%93%E5%87%BA%EF%BC%88SMI%EF%BC%89%E5%92%8C%E5%85%B7%E6%9C%89%E4%BC%A0%E6%84%9F%E5%99%A8%E7%9A%84%E5%8D%95%E6%A8%A1%E6%80%81%E8%BE%93%E5%87%BA%EF%BC%88SMS%EF%BC%89%E3%80%82MM%E3%80%81SMI%E5%92%8CSMS%E7%9A%84%E7%BB%93%E6%9E%9C%E7%84%B6%E5%90%8E%E8%A2%AB%E8%BE%93%E5%85%A5%E5%88%B0Pooler%E6%A8%A1%E5%9D%97%E4%B8%AD%EF%BC%8C%E4%BB%A5%E5%87%8F%E5%B0%8F%E7%89%B9%E5%BE%81%E7%9A%84%E7%BB%B4%E5%BA%A6%E3%80%82%E6%9C%80%E5%90%8E%EF%BC%8C%E8%BF%99%E4%BA%9B%E7%89%B9%E5%BE%81%E8%A2%AB%E8%BE%93%E5%85%A5%E5%88%B0%E7%BA%BF%E6%80%A7%E5%B1%82%E6%A8%A1%E5%9D%97%EF%BC%8C%E4%BB%A5%E8%BE%93%E5%87%BA%E5%8F%B6%E9%9D%A2%E7%A7%AF%E6%8C%87%E6%95%B0%EF%BC%88LAI%EF%BC%89%E7%9A%84%E5%80%BC%EF%BC%88%E5%9C%A8%5B0%2C1%5D%E8%8C%83%E5%9B%B4%E5%86%85%EF%BC%89%E3%80%82&width=1358 "图1. ViST用于生长预测的框架。实线表示正向传递，虚线表示循环。ViST的输入是传感器和图像数据。它们分别在MLP和LPFP模块中独立处理。来自两个模块的特征被输入到一个Transformer编码器中进行特征融合。编码器的输出被提供给具有多模态输出的Concat模块（MM）。同时，这些特征被分别发送到其他两个Transformer编码器中进行自注意力机制。这两个Transformer编码器的输出分别是具有图像的单模态输出（SMI）和具有传感器的单模态输出（SMS）。MM、SMI和SMS的结果然后被输入到Pooler模块中，以减小特征的维度。最后，这些特征被输入到线性层模块，以输出叶面积指数（LAI）的值（在[0,1]范围内）。")
#### MLP模块
一般来说，图像数据具有RGB三通道，每个像素具有一个值。但传感器数据只有几十个值。因此，传感器数据的值数量远远少于图像数据。如果直接整合这两种数据类型，图像数据将占主导地位，同时传感器数据将无法很好地表现出来。为了解决这个问题，将传感器数据转化为特征图。MLP模块可以增强传感器数据的特征。传感器数据由天气数据和土壤数据组成。这些数据是数值的，共有19个数据项。在数据预处理之后，传感器数据被排列成一维向量，并输入到多层前馈感知器（MLP）模块中。MLP模块的具体结构如图2所示。在MLP模块的输出之后，生成了一个包含768个元素的一维向量，导致向量大小为1×768。然后，使用以下方程将这个向量转化为一个145×768的矩阵：
$M_{out}=W_s\times M_{in}$(1)
其中，$M_{in}$是由MLP模块生成的1x768向量，然后被转换为一个145×768的矩阵，表示为$𝑀_{𝑜𝑢𝑡}$。$𝑊_𝑠$是一个145x1矩阵，其元素是通过训练模型获得的。
![图2. 传感器输入模型结构。MLP模块是一个多层感知器。它在输入层有19个神经元，在输出层有768个神经元。两个隐藏层分别包含32和64个神经元。我们发现使用2层MLP和3层MLP在性能上没有明显差异，因此我们使用了更简单的2层MLP。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697275890712-9778a481-4bca-4d01-943a-e4520d6e326b.png#averageHue=%23fbfbfa&clientId=uc9287c01-9d87-4&from=paste&height=207&id=u17c96c05&originHeight=294&originWidth=698&originalType=binary&ratio=2&rotation=0&showTitle=true&size=30338&status=done&style=none&taskId=u8371ab3a-fa76-4d5e-9515-9f4dcf5f282&title=%E5%9B%BE2.%20%E4%BC%A0%E6%84%9F%E5%99%A8%E8%BE%93%E5%85%A5%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E3%80%82MLP%E6%A8%A1%E5%9D%97%E6%98%AF%E4%B8%80%E4%B8%AA%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%E3%80%82%E5%AE%83%E5%9C%A8%E8%BE%93%E5%85%A5%E5%B1%82%E6%9C%8919%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%8C%E5%9C%A8%E8%BE%93%E5%87%BA%E5%B1%82%E6%9C%89768%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83%E3%80%82%E4%B8%A4%E4%B8%AA%E9%9A%90%E8%97%8F%E5%B1%82%E5%88%86%E5%88%AB%E5%8C%85%E5%90%AB32%E5%92%8C64%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83%E3%80%82%E6%88%91%E4%BB%AC%E5%8F%91%E7%8E%B0%E4%BD%BF%E7%94%A82%E5%B1%82MLP%E5%92%8C3%E5%B1%82MLP%E5%9C%A8%E6%80%A7%E8%83%BD%E4%B8%8A%E6%B2%A1%E6%9C%89%E6%98%8E%E6%98%BE%E5%B7%AE%E5%BC%82%EF%BC%8C%E5%9B%A0%E6%AD%A4%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%E4%BA%86%E6%9B%B4%E7%AE%80%E5%8D%95%E7%9A%842%E5%B1%82MLP%E3%80%82&width=492 "图2. 传感器输入模型结构。MLP模块是一个多层感知器。它在输入层有19个神经元，在输出层有768个神经元。两个隐藏层分别包含32和64个神经元。我们发现使用2层MLP和3层MLP在性能上没有明显差异，因此我们使用了更简单的2层MLP。")
#### 线性投影扁平化块模块
在本论文中，传感器数据被映射到与LPFP模块的图像特征相同的维度，以便后续的特征融合操作。值得注意的是，基于MLP模块的传感器特征不需要位置信息，并且传感器数据的输入顺序可以任意打乱。
![图3. 线性投影扁平化块模块。线性投影扁平化块将图像分割成若干相等的小图像，并通过矩阵变换将图像特征图与传感器特征对齐。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697281729140-0f0231f3-ec05-4c41-902c-17fcdd00067f.png#averageHue=%23f7f6f6&clientId=uc9287c01-9d87-4&from=paste&height=185&id=uf2a612b2&originHeight=339&originWidth=967&originalType=binary&ratio=2&rotation=0&showTitle=true&size=19813&status=done&style=none&taskId=ud804e491-4b49-427d-93c6-85547cd4edf&title=%E5%9B%BE3.%20%E7%BA%BF%E6%80%A7%E6%8A%95%E5%BD%B1%E6%89%81%E5%B9%B3%E5%8C%96%E5%9D%97%E6%A8%A1%E5%9D%97%E3%80%82%E7%BA%BF%E6%80%A7%E6%8A%95%E5%BD%B1%E6%89%81%E5%B9%B3%E5%8C%96%E5%9D%97%E5%B0%86%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%88%90%E8%8B%A5%E5%B9%B2%E7%9B%B8%E7%AD%89%E7%9A%84%E5%B0%8F%E5%9B%BE%E5%83%8F%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87%E7%9F%A9%E9%98%B5%E5%8F%98%E6%8D%A2%E5%B0%86%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E5%9B%BE%E4%B8%8E%E4%BC%A0%E6%84%9F%E5%99%A8%E7%89%B9%E5%BE%81%E5%AF%B9%E9%BD%90%E3%80%82&width=528.5 "图3. 线性投影扁平化块模块。线性投影扁平化块将图像分割成若干相等的小图像，并通过矩阵变换将图像特征图与传感器特征对齐。")
线性投影扁平化块模块的结构如图3所示。该模块的输入是尺寸为CHANEL×HEIGHT×WIDTH（C×H×W）的图像。这个三维张量对应于一个RGB图像。在输入图像之前，图像被调整为3×384×384的尺寸，如图3中的输入所示。通过卷积操作，图像被分割成大小为12×12的768个块。卷积操作使用大小为32×32的卷积核进行。卷积操作的步幅为32。每个小图像被展平成144个一维向量，768个展平的一维向量依次连接成768×144的向量。这个768×144的向量被转置成144×768的向量，用于与传感器的特征向量进行融合。然后，这个144×768的特征向量与类令牌向量连接在一起，生成补丁嵌入（patch embedding）。
类令牌是一个专门的令牌，可以表示图像的类别信息。通常，它被添加到图像的嵌入表示中。类令牌可以通过编码一些所有图像共享的全局特征来提供额外的信息，帮助模型更好地理解图像的内容。
1D位置嵌入的位置信息被添加到补丁嵌入中，以保留位置信息。类令牌和1D位置嵌入参考了ViT[55]。图像中每个像素的位置信息可以被转化为高维向量，其中向量的每个维度表示图像的特定维度中像素的位置。这个高维向量可以与每个像素的特征向量（例如，颜色信息、纹理信息等）连接在一起，从而得到一个更高维度的特征向量。ViT已经证明，2D位置嵌入的性能不如1D位置嵌入，因此选择了1D位置嵌入。
最后，将补丁嵌入与1D位置嵌入相加，得到图像的特征图。
图像和传感器的特征都使用相同维度的向量表示。这些向量已准备好被输入到下一个Transformer编码器中进行完全的特征融合。
#### 用于融合的 Transformer 编码器
![图4. 用于融合的Transformer编码器（TEF）模型的结构由交替层的多头交叉注意力（MHCA）、多层感知器（MLP）和层归一化（LN）模块组成。MHCA是负责融合图像和传感器特征的核心模块。MLP是从ViT中采用的，而LN模块负责执行层归一化操作。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697282718889-93e3af57-aa42-45d9-8a92-0309a9b2e279.png#averageHue=%23e5bb69&clientId=uc9287c01-9d87-4&from=paste&height=445&id=u693dbf9d&originHeight=1236&originWidth=1582&originalType=binary&ratio=2&rotation=0&showTitle=true&size=300310&status=done&style=none&taskId=ua1cb4b6c-5abc-4b4b-b62a-64201e2d099&title=%E5%9B%BE4.%20%E7%94%A8%E4%BA%8E%E8%9E%8D%E5%90%88%E7%9A%84Transformer%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%88TEF%EF%BC%89%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%93%E6%9E%84%E7%94%B1%E4%BA%A4%E6%9B%BF%E5%B1%82%E7%9A%84%E5%A4%9A%E5%A4%B4%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88MHCA%EF%BC%89%E3%80%81%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%EF%BC%88MLP%EF%BC%89%E5%92%8C%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88LN%EF%BC%89%E6%A8%A1%E5%9D%97%E7%BB%84%E6%88%90%E3%80%82MHCA%E6%98%AF%E8%B4%9F%E8%B4%A3%E8%9E%8D%E5%90%88%E5%9B%BE%E5%83%8F%E5%92%8C%E4%BC%A0%E6%84%9F%E5%99%A8%E7%89%B9%E5%BE%81%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97%E3%80%82MLP%E6%98%AF%E4%BB%8EViT%E4%B8%AD%E9%87%87%E7%94%A8%E7%9A%84%EF%BC%8C%E8%80%8CLN%E6%A8%A1%E5%9D%97%E8%B4%9F%E8%B4%A3%E6%89%A7%E8%A1%8C%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E6%93%8D%E4%BD%9C%E3%80%82&width=569 "图4. 用于融合的Transformer编码器（TEF）模型的结构由交替层的多头交叉注意力（MHCA）、多层感知器（MLP）和层归一化（LN）模块组成。MHCA是负责融合图像和传感器特征的核心模块。MLP是从ViT中采用的，而LN模块负责执行层归一化操作。")
用于融合多模态数据的Transformer编码器（TEF）模型充当了核心模型的角色。如图4所示，TEF的计算过程如下所示。
$\begin{gathered}I_{attn},S_{attn}=MHCA\left(I_{in},S_{in}\right) \\I_{out}=MLP\left(LN\left(I_{in}\cdot I_{attn}\right)\right)+I_{in}\cdot I_{attn} \\S_{out}=MLP\left(LN\left(S_{in}\cdot S_{attn}\right)\right)+S_{in}\cdot S_{attn} \end{gathered}$
这里，$I_{𝑎𝑡𝑡𝑛}$和$S_{𝑎𝑡𝑡𝑛}$分别被定义为MHCA模块的输出。$I_{𝑎𝑡𝑡𝑛}$和$S{𝑎𝑡𝑡𝑛}$是具有注意力机制的图像和传感器特征图，分别。$𝐼_{𝑖𝑛}$和$S_{𝑖𝑛}$被分别定义为TEF的输入。$𝐼_{out}$和$S_{out}$分别是TEF的输出。$𝐼_{𝑖𝑛}$和$I_{out}$是图像特征图。$S_{𝑖𝑛}$和$S_{out}$是传感器特征图。TEF需要进行12轮的计算。TEF的输出$𝐼_{out}$和$S_{out}$将成为下一轮TEF的输入。在完成12轮后，$𝐼_{out}$和$S_{out}$将传递给下一个模块。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697283472170-c1b440e6-e2f3-498c-9ef1-14140ba68811.png#averageHue=%23fae6c1&clientId=uc9287c01-9d87-4&from=paste&height=432&id=ub3795e62&originHeight=864&originWidth=1433&originalType=binary&ratio=2&rotation=0&showTitle=false&size=104150&status=done&style=none&taskId=u1a781129-93b3-4a62-bd2f-b3cd4e44432&title=&width=716.5)
图5. 多头交叉注意力模块（MHCA）的结构充当了Transformer编码器用于融合的核心组件。它通过接受图像特征图（$𝐼_{𝑖𝑛}$）和传感器特征图（$S_{𝑖𝑛}$）作为输入进行操作。该模块使用交叉注意力机制来研究每个特征图之间以及两个特征图之间的相互关系和关联。通过这种方式，MHCA模块实现了从两个模态中识别有意义特征的改进能力，生成具有注意力的图像特征图（$I_{𝑎𝑡𝑡𝑛}$）和具有注意力的传感器特征图（$S{𝑎𝑡𝑡𝑛}$）作为输出。
交叉注意力模块被提出来建模图像和传感器特征的内部模态关系。MHCA的结构如图5所示。有两个分支用于确定参数Query（𝑄, 𝑄′）、Key（𝐾, 𝐾′）和Value（𝑉, 𝑉′）。一个分支计算了用于具有注意力的图像特征图$𝐼_{𝑎𝑡𝑡𝑛}$的𝑄, 𝐾, 𝑉。另一个分支计算了用于具有注意力的传感器特征图$𝑆_{𝑎𝑡𝑡𝑛}$的𝑄′, 𝐾′, 𝑉′。计算过程如下所示：
$\begin{gathered} Q=S_{in}W_{q},K=I_{in}W_{k},V=I_{in}W_{\upsilon} \\ A=softmax\left(\frac{QK^T}{\sqrt{C/h}}\right)V \\ Q'=I_{in}W_{q}',K'=S_{in}W_{k}',V'=S_{in}W_{v}' \\ \begin{aligned}A'&=softmax\left(\frac{Q'K'^T}{\sqrt{C/h}}\right)V'\end{aligned} \end{gathered}$
其中，$W_q,W_k,W_v,W_q^{\prime},W_k^{\prime},W_k^{\prime}\in\mathbb{R}^{C\times(C/h)}$ 是具有可学习参数的线性变换矩阵。这些多个矩阵是通过多头机制计算的。C和h分别是嵌入维度和头数。A和𝐴′分别是方程6和方程8的结果。多个A和𝐴′分别进行连接。一个分支中的传感器特征图𝑆𝑖𝑛用作查询𝑄来计算𝐾′，然后更新𝑉 ′。另一个分支中的图像特征图$𝐼_{𝑖𝑛}$用于查询𝑄′来计算键𝐾和值𝑉。因此，这两个分支在语义信息共享方面交换传感器和图像特征之间的信息。在交叉注意力中，生成注意力图的计算复杂度是线性的，而不是全注意力中的二次复杂度。MHCA模块的整个过程更加高效，可以降低过拟合的风险。
#### 损失函数
叶面积指数（LAI）数据是从农场手动收集的，用作ViST模型的标签。在数据归一化之后，LAI值被缩放到[0,1]范围。损失函数度量了模型的预测值𝑦′与实际值𝑦之间的差异。这是一个非负实值函数，通常表示为𝐿(𝑦, 𝑦′)，它构成了经验风险函数和结构风险函数的基础。
在本文中，我们将作物生长预测视为回归问题。通常，用具有单个输出节点的神经网络来解决回归问题，输出值表示预测值。为了评估我们模型的性能，我们使用均方误差损失函数作为主要指标。
$MSE\left(y,y^{\prime}\right)=\frac{\sum_{i=1}^n\left(y_i-y_i^{\prime}\right)^2}n$
其中，𝑛是样本数量，$𝑦_𝑖$是第i个样本的叶面积指数的真实值，$y_{i}^{\prime}$是第i个样本的叶面积指数的预测值。均方误差（MSE）用作我们模型输出的精度度量，用于在后续迭代中更新模型参数。
### 实验
在本节中，我们将介绍用于数据收集的方法，描述所使用的设备和数据格式。在数据收集之后，图像和传感器数据都经过预处理。还将概述模型性能的评估标准和实验细节。
#### 数据收集
数据采集的位置如图6所示。水稻、玉米和大豆的数据是在农场上收集的。每种作物有三个样本点。总共有9个样本点在农场上。数据采集设备如图7所示。农场上的每个样本点都放置了一个设备，包括一个摄像头和11个传感器。LAI数据是定期使用手持设备在每个样本点上收集的，以进行数据对齐。作物是从顶视角拍摄的。农场上摄像头捕获的水稻、大豆和玉米的图像分别如图8(a)(b)(c)所示。高度设置为3米。图像格式为RGB，分辨率为3840×2160。每张图像之间的时间间隔为两小时。所收集的传感器项目如表1所示，包括十一个传感器数据类型。土壤传感器分别部署在地下的10cm、20cm、30cm、40cm和50cm深度。空气、光线和风速传感器部署在IoT设备的顶部。每个传感器的数据收集时间间隔为半小时。图像和传感器数据定期收集并上传到云服务器进行存储和数据分析。
#### 数据处理
**图像数据的预处理**：为了确保在反向传播中更好的收敛性，对每幅图像的数据使用Z分数方法进行归一化，将数据归一化到特定范围。

$z=\frac{x-\mu}\sigma$

$z=\frac{x-\mu}\sigma$
在本研究中，其中𝜇表示均值，𝜎表示标准差，𝑥表示输入数据，𝑧表示输出数据。分别记录了图像三个通道的均值和标准差。具体来说，将规范化图像的每个通道的均值设置为0，方差设置为1。但需要注意的是，这种归一化方法可能不适用于样本量较小的情况，一般建议仅在样本量超过30时使用。表2展示了水稻、大豆和玉米图像计算的规范化结果。
**传感器数据的预处理**：为了消除尺寸之间的影响，我们使用以下方程对每个尺寸进行了归一化，因为传感器数据中各尺寸之间的数值差异相对较大。
$x_{normalized}=\frac{x-x_{min}}{x_{max}-x_{min}}$
其中，$x_{min}$是最小值，$x_{max}$是最大值。$𝑥_{𝑛𝑜𝑟𝑚𝑎𝑙𝑖𝑧𝑒𝑑}$是归一化后的数值。
当用于处理实际数据时，每个指标的最大值和最小值被保存在一个单独的文件中，用于数据预处理。处理后的数据如表3所示。
表2. 图像标准化数据。该表呈现了三种作物（水稻、大豆和玉米）的图像RGB通道的均值和标准差值。这些值是通过前面部分描述的规范化过程获得的。
![CleanShot 2023-10-14 at 20.21.31@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697286099049-834f6412-b9fc-4440-adfd-8779140b1581.png#averageHue=%23efefef&clientId=uc9287c01-9d87-4&from=paste&height=330&id=uf94e93f3&originHeight=744&originWidth=1124&originalType=binary&ratio=2&rotation=0&showTitle=false&size=110974&status=done&style=none&taskId=u8c448331-63f2-4d28-8d67-cfc3f1e6a6c&title=&width=498)
表3. 传感器输入指标的预处理。该表包括传感器数据处理的指标，以及它们的最大和最小值。
![CleanShot 2023-10-14 at 20.41.07@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697287276999-595397e1-3dc5-445e-9e6a-9a9ee31f386f.png#averageHue=%23ededed&clientId=uc9287c01-9d87-4&from=paste&height=548&id=uf474c586&originHeight=1204&originWidth=1350&originalType=binary&ratio=2&rotation=0&showTitle=false&size=263018&status=done&style=none&taskId=u9ae5c964-8960-4a70-9e2e-75954fdd759&title=&width=615)
**LAI数据的处理：**为了确保在反向传播中更好的收敛性，使用Z分数方法对图像数据进行了归一化，将数据归一化到特定范围。对于每种作物，三组图像和传感器设备放置在测量LAI时与相机的相同位置。在LAI数据收集过程中，手持设备捕捉相机设备覆盖的部分区域。LAI数据每5天收集一次。为了获取更多的数据，使用了分段三次埃尔米特插值多项式(piecewise cubic Hermite interpolation polynomial (PCHIP) )方法，如Fritsch和Carlson[56]所述。LAI数据每天都可用，并对水稻、大豆和玉米的三个样本点进行插值，如图9(a),(b),(c)所示。图的左侧显示了原始数据，右侧显示了插值数据。插值数据看起来更平滑，从曲线中可以看出。
![水稻LAI插值前后](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697287432759-e366d76b-1105-4900-a91f-6c283f991fea.png#averageHue=%23faf7f6&clientId=uc9287c01-9d87-4&from=paste&height=328&id=ub764d341&originHeight=656&originWidth=2026&originalType=binary&ratio=2&rotation=0&showTitle=true&size=293800&status=done&style=none&taskId=u00b8a312-d839-45cf-a711-e0a953049e1&title=%E6%B0%B4%E7%A8%BBLAI%E6%8F%92%E5%80%BC%E5%89%8D%E5%90%8E&width=1013 "水稻LAI插值前后")
![大豆LAI插值前后](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697287448877-72b6bdb8-345e-40bd-90dc-e0d8ee44c91d.png#averageHue=%23f9f7f2&clientId=uc9287c01-9d87-4&from=paste&height=308&id=uf7bc0c0a&originHeight=616&originWidth=2036&originalType=binary&ratio=2&rotation=0&showTitle=true&size=271511&status=done&style=none&taskId=u7b1db0ec-f748-4486-b2c6-65bffc4669a&title=%E5%A4%A7%E8%B1%86LAI%E6%8F%92%E5%80%BC%E5%89%8D%E5%90%8E&width=1018 "大豆LAI插值前后")
![玉米LAI插值前后](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697287457482-287c10c8-9a10-4f2d-b62e-26e8fd6615d1.png#averageHue=%23fafafa&clientId=uc9287c01-9d87-4&from=paste&height=302&id=ub42440ee&originHeight=604&originWidth=2104&originalType=binary&ratio=2&rotation=0&showTitle=true&size=226060&status=done&style=none&taskId=u4bb5ca97-af44-43ee-a356-989fa32b50e&title=%E7%8E%89%E7%B1%B3LAI%E6%8F%92%E5%80%BC%E5%89%8D%E5%90%8E&width=1052 "玉米LAI插值前后")
图9. LAI数据预处理。图中显示了插值前后的叶面积指数数据。横轴表示时间，纵轴表示叶面积指数。
**数据对齐：**一个样本包括图像数据、传感器数据和一个标签 - LAI数据。基于各自的采样位置和时间，对图像数据、传感器数据和LAI数据进行了对齐。每幅图像是以两小时的间隔捕获的，而传感器的采样频率为每半小时一次。数据集之间的时间对齐是以半小时为基础建立的。由于相对于传感器数据，图像数据的数量较少，因此在两小时内收集的四个样本对应一幅图像。值得注意的是，每天只有一个LAI数据点；这意味着有四十八个样本共享相同的LAI标签。
训练集和测试集：对于每种作物，选择了三个样本点，在农场上共有九个观测点。其中两个样本点用于模型训练，而剩余的一个样本点用于模型测试。针对每种作物，跨不同样本点进行交叉验证，以防止在训练和测试中使用相同的样本数据，这可能导致数据梯度的差异较小。
实验中使用的数据量如图4所示，其中“days”表示数据收集的持续时间。“Train sample”和“Test sample”分别指的是用于训练和测试的数据实例数量。
表4. 实验数据的数量。
![CleanShot 2023-10-14 at 20.48.47@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697287734540-372eb2f4-6982-47d1-bcc1-36c61f548a2e.png#averageHue=%23ededed&clientId=uc9287c01-9d87-4&from=paste&height=167&id=u8ee41ba2&originHeight=334&originWidth=978&originalType=binary&ratio=2&rotation=0&showTitle=false&size=43275&status=done&style=none&taskId=uf46d4c1d-41ec-48de-9094-04619605b46&title=&width=489)

#### 评估指标
均方误差（MSE）、平均绝对误差（MAE）、平均绝对百分比误差（MAPE）和对称平均绝对百分比误差（SMAPE）用于预测各种模型的性能。MSE与损失函数相同。MAE是目标值和预测值之间绝对差的总和。
$MAE=\frac{\sum_i^n\left|y_i-y_i^{\prime}\right|}n$
其中，$y_i^{\prime}$是模型的评估值，$𝑦_𝑖$是真实值，𝑛是样本总数。均方误差（MSE）是模型评估值与实际样本值𝑦之间差的平方的平均值。其公式如下：
$MSE=\frac{\sum_{i=1}^n\left(y_i^{\prime}-y_i\right)^2}n$
其中，$𝑦_𝑖$和$y_i^{\prime}$，i分别是第一个样本的真实值和相应的评估值，𝑛是样本数量。MAPE是估计中常用的性能度量标准。它通过计算估计值$y_i^{\prime}$与实际值$𝑦_𝑖$之间的绝对百分比差异来衡量估计的准确性，然后取所有绝对百分比差异的平均值。计算MAPE的公式如下：
$MAPE=\frac{100\%}n\sum_{i=1}^n\left|\frac{y_i^{\prime}-y_i}{y_i}\right|$
其中，𝑛是样本数量，$𝑦_𝑖$是实际值，$y_i^{\prime}$是估计值。SMAPE是评估预测模型准确性的广泛使用的度量标准。它是一种对称误差，计算实际值和评估值之间的平均百分比差异。与其他误差度量不同，SMAPE考虑了实际值的幅度，使其成为比较不同预测模型准确性的更可靠的度量标准。计算SMAPE的公式如下：
$SMAPE=\frac{100\%}n\sum_{i=1}^n\frac{\left|y_i^{\prime}-y_i\right|}{\left(\left|y_i^{\prime}\right|+\left|y_i\right|\right)/2}$
其中，𝑛、$𝑦_𝑖$和$y_i^{\prime}$与MAPE公式中的符号相同。SMAPE的范围从0%到200%，数值越低表示准确性越好。
#### 实验和结果
在这一部分，我们进行了广泛的实验，以展示我们提出的ViST模型相对于现有方法的有效性。我们的模型与最先进的模型进行了比较，如Rice-Fusion [57]、RiceTransformer [58]、CNN-Transformer [59]、DNNF1和DNNF2 [49]。虽然DNNF1和DNNF2都是用于大豆产量预测的多模态模型，但Rice-Fusion和Rice-Transformer采用了多模态方法来诊断水稻疾病。CNN-Transformer模型是基于多时相数据对作物进行分类的。在比较方面，这些模型的输入和输出与我们的ViST模型相同。最后，所有模型的输出都基于代表作物生长状态的LAI值进行评估。
ViST模型进行了去除实验，涉及其输入模式，包括仅图像数据、仅传感器数据和多模态数据。使用一系列指标对每个模型的性能进行了评估，包括MAE、MSE、MAPE和SMAPE，它们分别在方程12、13、14和15中呈现。
预处理后的数据被用来训练每个模型，使用了ViST的主要超参数，这些超参数列在表5中。采用了Adam优化器，权重衰减为0.0001，并且所有模型都在GeForce RTX 3090 GPU上进行训练。为了动态调整学习率，实施了余弦退火策略。最大迭代次数是根据样本大小和时代确定的，在训练过程中，学习率随着时代的增加而单调下降。ViST模型的主要超参数也可以在表5中找到。使用随机正态分布来初始化每个模型的参数。在去除实验中，MM、SMI和SMS被用作模型的输入。
表5. 模型的主要超参数
![CleanShot 2023-10-14 at 21.05.18@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697288724954-b9fde838-08b4-402f-93c7-878281d7d495.png#averageHue=%23eeeeee&clientId=uc9287c01-9d87-4&from=paste&height=349&id=u09103137&originHeight=698&originWidth=662&originalType=binary&ratio=2&rotation=0&showTitle=false&size=69099&status=done&style=none&taskId=u581d03d0-8403-4269-b05a-efb0166ff6c&title=&width=331)
##### 针对单一作物的实验和结果
在本节中，我们评估了我们的模型在三种作物——水稻、大豆和玉米上的性能。首先，我们比较了使用单模态数据和多模态数据的模型性能，如A节中所讨论的。其次，我们将我们的ViST模型的性能与其他模型的性能进行了比较，如B节中所概述的。
**A. ViST模型在单模态数据和多模态数据下的性能比较**
ViST模型针对每种作物使用了三种不同的数据输入模式进行训练：图像数据、传感器数据、图像和传感器数据的组合。因此，每种作物都得到了三个训练好的模型。
表6. 不同输入模式下的ViST模型测试结果。
![CleanShot 2023-10-14 at 22.09.06@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697292553951-819fe5b4-5bc2-491b-90e9-abcb04abfd6d.png#averageHue=%23e6e6e6&clientId=uc9287c01-9d87-4&from=paste&height=291&id=u53f3c525&originHeight=708&originWidth=1424&originalType=binary&ratio=2&rotation=0&showTitle=false&size=141836&status=done&style=none&taskId=ua50200a4-f5d5-4877-86fc-d5ba348ece1&title=&width=585)
在获得训练模型之后，我们使用测试数据集中的数据来测试这些模型。ViST模型在测试数据集上的性能如表6所示。
结果显示，在三种作物的三种模式中，多模态模式在预测指标方面取得了最低值。由于四个指标在实验结果上呈现相同趋势，因此采用MAE来分析ViST模型的性能。在水稻实验中，与传感器和图像模式相比，MAE值分别减少了61.25%、74.49%。在大豆实验中，MAE值分别减少了97.13%、92.94%。玉米实验中，MAE值分别减少了90.45%、93.35%。因此可以得出结论：将图像和传感器数据结合起来显著提高了ViST模型的性能，并且传感器和图像数据对于模型训练具有几乎相等的贡献。
对于水稻而言，传感器比图像更有效果，因为水稻生长过程中图像变化很小且叶片特征不明显；对于大豆而言，则是图像比传感器更有效果，因为其生长过程中图像特征发生明显变化；至于玉米，则是图像和传感器数据差异不大，两者都能有效反映作物生长情况，因为各种评估指标的误差率相对较小。
利用图像分析可以分析作物表面纹理特征，如颜色、质地和形状，以推断作物的健康状况，包括是否生长良好以及是否受到害虫和疾病的影响。传感器数据可以捕捉环境因素（如温度、湿度、光线和土壤）的变化，并评估其对作物生长的影响。例如，过度干旱或过度湿润会对作物生长产生负面影响。通过综合分析这两个方面的信息，可以更准确地评估作物生长情况，并相应调整适当的农业管理措施。
在水稻、大豆和玉米实验中，多模态结果优于单模态结果。多模态数据可以补充单模态数据中缺失的信息，填补信息空白。例如，图像只能反映作物表面信息，而传感器只能反映环境信息。如果仅使用单一模态数据，则会忽略许多重要信息。通过整合多种类型的信息，在数据分析方面不同类型的信息可以互补彼此，提高准确性和全面性。
图像和传感器多模态数据的融合可以完成噪声抑制。数据上的噪声是常见的，尤其在传感器数据处理中。然而，通过融合数据，多模态数据可以减少数据上的噪声，这比单一模态数据分析更具鲁棒性。
单一模态数据比多模态数据更容易受到干扰，后者更具鲁棒性。传感器数据通常会受到各种外部环境因素（如雨水、灰尘和遮挡）的影响，这可能会影响到数据准确性。相反，图像数据受外部因素的影响较小。因此，在多模态 数据分析中，图像 数据与其他类型 的组合显著提高了数据准确性和鲁棒性。
因此，多模态数据分析的互补性、噪声抑制和鲁棒性可以更全面地理解物体的本质并更准确地描述实际情况。
**B. 与其他模型的比较**
在本节中，我们将提出的ViST模型与其他模型进行了比较：DNNF1、DNNF2、CNN-Transformer、Rice-Fusion和Rice-Transformer。这三个模型分别在稻谷、大豆和玉米数据上进行了训练。
表7：水稻比较实验测试结果。“平均值”和“标准差”分别表示从五个实验组得到的结果的平均值和标准差。
![CleanShot 2023-10-14 at 22.35.53@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697294164487-0ae746f9-6606-4633-929d-35786c459625.png#averageHue=%23e6e6e6&clientId=ue8499c17-ada4-4&from=paste&height=115&id=u1f9d4810&originHeight=230&originWidth=958&originalType=binary&ratio=2&rotation=0&showTitle=false&size=55906&status=done&style=none&taskId=u5c154b00-afaa-4c7f-b1d6-0b89aa8a4d8&title=&width=479)
表7显示了将各种模型应用于稻谷数据集时的实验结果。ViST模型在所有预测指标上都展现出最佳性能，这意味着它在估计稻谷生长方面具有出色的表现。平均而言，ViST将MSE降低了55.80％，MAE降低了38.48％，MAPE降低了44.21％，SMAPE降低了42.59％。与其他模型相比，ViST模型在准确性和稳定性方面表现更好，尤其是与DNNF1和DNNF2相比较。例如，与DNNF1相比，ViST将MSE降低了87.01％、MAE降低了48.00％、MAPE降低了61.83％、SMAPE降低了56.24％。同时还显示出ViST在MSE上几乎与CNN-Transformer和Rice-Fusion模型具有相同的值。
与表7类似，表8展示了ViST和其他比较模型的实验结果。表8中的实验使用大豆数据作为数据集。通过考虑相对于其他模型的改进比率，可以明显看出ViST在所有四个指标上都取得了显著的提升，其中在MAE和MAPE指标上改进最为明显。关于MSE指标，相对于DNNF1、DNNF2、Rice-Fusion和Rice-Transformer，ViST分别实现了56.25%、92.31%、61.11%和50.00%的减少。此外，相对于CNN-Transformer，ViST将误差降低了46.15%。
表9显示了将各种模型应用于玉米数据集时的实验结果。Rice-Fusion模型在四个性能指标上表现良好，尽管在MAPE
表8方面略逊于ViST。大豆的比较实验测试结果。“Mean”和“Std”分别表示从五个实验组得到的结果的平均值和标准偏差。
![CleanShot 2023-10-14 at 22.51.20@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697295099495-9c8c1576-6c55-49bf-8770-d5394ffca0ac.png#averageHue=%23e7e7e7&clientId=ue8499c17-ada4-4&from=paste&height=119&id=ucd4da0e7&originHeight=238&originWidth=976&originalType=binary&ratio=2&rotation=0&showTitle=false&size=57145&status=done&style=none&taskId=ue42caeae-57e0-49ed-ac0c-803024bd453&title=&width=488)
表9. 玉米的比较实验测试结果。"平均值"和"标准差"分别表示从五个实验组得到的结果的平均值和标准差。
# 附录
## 1. random_split
`random_split`函数通常用于数据集的交叉验证或训练集/验证集/测试集的划分，以便在模型训练和评估过程中使用不同的数据集。该函数可以帮助确保模型在不同的数据子集上进行训练和评估，从而提高模型的泛化能力和鲁棒性。
在下面的代码中，使用`random_split`函数将数据集（`data_set`）随机划分为训练集和验证集。`random_split`函数接受三个参数：要划分的数据集，一个包含训练集和验证集大小的列表，以及一个随机数生成器（通过`torch.Generator().manual_seed(seed)`创建）。
`torch.Generator().manual_seed(seed)`: 这是一个用于生成随机数的生成器，通过`seed`参数设置种子以确保每次划分的结果相同。通过设置相同的种子，可以在不同的运行中得到相同的训练集和验证集划分。
`return np.array(train_set), np.array(valid_set)`: 返回划分后的训练集和验证集。使用`np.array`将PyTorch的`Dataset`对象转换为NumPy数组。
```python
def train_valid_split(data_set, valid_ratio, seed):
    '''
    数据集拆分成训练集（training set）和 验证集（validation set）
    '''
    valid_set_size = int(valid_ratio * len(data_set)) # 验证集大小
    train_set_size = len(data_set) - valid_set_size # 训练集大小
    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))
    return np.array(train_set), np.array(valid_set)
```
## 2. matplotlib
### 2.1 bar（条形图）
比较不同类别和组之间的数据，是离散的（**不挨着**）。
### 2.2 hist（直方图）
展示数据的分布情况。直方图通过将数据划分为多个**离散的区间**（也称为“**bin**”），并计算每个区间内数据点的频数或频率，然后绘制柱状图来表示。直方图的 x 轴代表**数据的范围**，y 轴代表每个区间内数据的**频数或频率**。
### 2.3 matplotlib保存不同的图
在绘制第二个图之前调用 `plt.clf()` 或 `plt.figure()` 来清空画布，或者使用 `subplot` 来创建多个子图。以下是修改后的代码示例：

1. plt.clt()
```python
# label分布情况
plt.figure()  # 创建新的画布
ax1 = dftrain_raw['Survived'].value_counts().plot(
    kind='bar',
    figsize=(12, 8),
    fontsize=14,
    rot=0
)
ax1.set_ylabel('Counts', fontsize=14)
ax1.set_xlabel('Survived', fontsize=14)
plt.savefig('./数据分析/标签分布LabelCounts.png')

# 清空画布
plt.clf()

# 年龄分布情况
plt.figure()  # 创建新的画布
ax2 = dftrain_raw['Age'].plot(
    kind='hist',
    bins=20,
    color='purple',
    figsize=(12, 8),
    fontsize=14
)
ax2.set_ylabel('Frequency', fontsize=14)
ax2.set_xlabel('Age', fontsize=14)
plt.savefig('./数据分析/年龄分布AgeDistribution.png')
```

2. subplot
```python
# 创建一个包含两个子图的画布
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 16))

# 绘制第一个图（标签分布）
dftrain_raw['Survived'].value_counts().plot(
    kind='bar',
    ax=ax1,
    fontsize=14,
    rot=0
)
ax1.set_ylabel('Counts', fontsize=14)
ax1.set_xlabel('Survived', fontsize=14)

# 绘制第二个图（年龄分布）
dftrain_raw['Age'].plot(
    kind='hist',
    bins=20,
    color='purple',
    ax=ax2,
    fontsize=14
)
ax2.set_ylabel('Frequency', fontsize=14)
ax2.set_xlabel('Age', fontsize=14)

# 调整子图间的间距
plt.subplots_adjust(hspace=0.3)

# 保存图像
plt.savefig('./数据分析/标签和年龄分布.png')
```
## 3. nn.BCEWithLogitsLoss()
BCE 表示二元交叉熵 (Binary Cross Entropy)，Logits 表示模型的输出层没有经过激活函数（例如 Sigmoid），因此这个函数将会自动应用 Sigmoid 激活函数并计算二元交叉熵损失。
## 4. super(XunFeiNet, self).init()与super().init()
在Python中：

- `super().__init__()`会自动推断当前类和实例，并调用父类的构造函数。
- `super(XunFeiNet, self).__init__()`则显式地指定了当前类名和实例，以调用父类的构造函数。

## 5. Pytorch的图片默认顺序是 Batch,Channel,Width,Height
