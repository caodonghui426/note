# 1.引言

基于混合专家的架构（Mixture-of-Experts，MoE）[3, 4, 5]已经开辟了一条有前途的道路，使得模型参数的计算要求具有次线性特性，并能够在不增加训练成本的情况下改进模型质量。然而，基于MoE的模型具有**一系列挑战**，限制了它们在广泛的实际场景中的应用：

**有限的范围**：在自然语言处理领域，MoE（Mixture of Experts）基于模型的范围主要局限于编码器-解码器模型和序列到序列任务，对于在其他领域中应用它的工作相对较少。至于将MoE应用于自回归的自然语言生成（NLG），比如GPT-3和MT-NLG 530B，在训练最先进的语言模型的计算成本可以比编码器-解码器模型高出几个数量级时，研究相对较少。

**大规模的内存需求**：虽然MoE模型在实现相同的模型质量方面所需的计算量较少，但需要更多数量的参数。例如，基于MoE的Switch-Base模型的参数比T5-large多10倍（7.4B比0.74B），然而，在广泛的下游任务中进行比较时，其模型质量仍然不同。换句话说，与质量等效的密集模型相比，基于MoE的模型具有更低的"**参数效率**"。更大的模型尺寸和较低的参数效率对训练和推理都带来了挑战。

对于训练来说，模型尺寸的大幅增加需要相应增加设备内存。请注意，上述提到的T5-large（0.74B）可以适应单个32GB V100 GPU的内存，而训练Switch-Base（7.4B）则需要至少8-10个这样的GPU才能将模型适应设备内存进行训练。如果我们将密集模型尺寸按照500B参数缩放到MT-NLG等效模型，使用MoE基于模型可能需要一个具有超过5万亿参数的模型来实现类似的质量（假设10倍的缩放仍然成立），这将需要超过5K个GPU才能仅仅适应模型状态进行训练。这个庞大的模型尺寸使得基于MoE的模型在规模上的训练具有挑战性，不仅需要设备上的内存，还需要系统支持来有效利用成千上万个GPU上的设备内存。

**受限的推理性能**：由于上述大模型尺寸和参数效率低下，基于MoE的模型的快速推理变得更加具有挑战性。一方面，更大的参数尺寸需要更多的GPU来适应，而多GPU推理技术并不适用于基于MoE的模型。另一方面，由于推理通常受内存带宽限制，相比于密集模型，MoE模型可能需要10倍更高的可实现内存带宽才能实现类似的推理延迟。



