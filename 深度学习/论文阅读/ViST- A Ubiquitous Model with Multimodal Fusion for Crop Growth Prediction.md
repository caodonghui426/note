# 1.摘要
作物生长预测可以帮助农业工作者在农业活动中做出准确和合理的决策。现有的作物生长预测模型主要关注单一作物，并为每种作物训练单独的模型。在本文中，我们将开发一个适用于多种作物的普适生长预测模型，旨在为多种作物训练一个单一模型。我们开发了一种适用于作物生长预测的普适视觉和传感器变换器（ViST）模型，该模型利用图像和传感器数据。在所提出的模型中，提出了一种交叉注意力机制，以实现多模态特征图的融合，以降低计算成本并平衡特征之间的相互作用效果。为了训练该模型，我们将来自多种作物的数据结合起来，训练一个单一的（ViST）模型。在种植稻谷、大豆和玉米的农场上构建了一个传感器网络系统，用于数据收集。实验结果表明，所提出的ViST模型具有出色的适用性，可用于多种作物的生长预测。
# 2.导言
# 3.结论
本论文提出了一种基于Transformer的ViST模型，用于利用图像和传感器数据在农场进行作物生长预测。模型中的交叉注意力机制被用来改善模型数据融合的效果。三种作物的数据被一起训练作为模型的输入。该模型不仅实现了高准确性，而且保持了相对较快的速度。实验结果显示，多模态数据的模型可以提高作物生长预测的效果。尽管ViST模型取得了良好的结果，但仍存在一些限制。由于模型需要进行跨模态特征融合，这可能会增加模型的计算成本和训练时间。模型的广泛适用性需要在不同作物上进行进一步验证，以确定其适用性和通用性。在未来，将收集更多的作物生长数据以进行模型优化。此外，研究该模型在实际农业生产中的应用可以帮助农民更好地决策和生产管理。
# 4.相关工作
# 5.模型
所提出的ViST模型的整体网络结构如图1所示。输入包括作物图像和传感器数据。在这个框架中，MLP模块和线性投影Flattened Patches模块（LPFP）分别从传感器数据和图像数据中提取特征。LPFP是由ViT[55]引入的。变压器编码器用于数据融合。Pooler和Linear模块旨在减小特征的维度。框架中每个模块的具体细节如下所述。
![图1. ViST用于生长预测的框架。实线表示正向传递，虚线表示循环。ViST的输入是传感器和图像数据。它们分别在MLP和LPFP模块中独立处理。来自两个模块的特征被输入到一个Transformer编码器中进行特征融合。编码器的输出被提供给具有多模态输出的Concat模块（MM）。同时，这些特征被分别发送到其他两个Transformer编码器中进行自注意力机制。这两个Transformer编码器的输出分别是具有图像的单模态输出（SMI）和具有传感器的单模态输出（SMS）。MM、SMI和SMS的结果然后被输入到Pooler模块中，以减小特征的维度。最后，这些特征被输入到线性层模块，以输出叶面积指数（LAI）的值（在[0,1]范围内）。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697338290691-ecd40e35-f4a5-43de-8284-3a5ce190b93e.png#averageHue=%23e5ba84&clientId=u8f1acba0-7063-4&from=paste&height=703&id=KN8Er&originHeight=1406&originWidth=2716&originalType=binary&ratio=2&rotation=0&showTitle=true&size=610543&status=done&style=none&taskId=uc3a624c0-613c-44e3-ba38-ed2685fbe30&title=%E5%9B%BE1.%20ViST%E7%94%A8%E4%BA%8E%E7%94%9F%E9%95%BF%E9%A2%84%E6%B5%8B%E7%9A%84%E6%A1%86%E6%9E%B6%E3%80%82%E5%AE%9E%E7%BA%BF%E8%A1%A8%E7%A4%BA%E6%AD%A3%E5%90%91%E4%BC%A0%E9%80%92%EF%BC%8C%E8%99%9A%E7%BA%BF%E8%A1%A8%E7%A4%BA%E5%BE%AA%E7%8E%AF%E3%80%82ViST%E7%9A%84%E8%BE%93%E5%85%A5%E6%98%AF%E4%BC%A0%E6%84%9F%E5%99%A8%E5%92%8C%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E3%80%82%E5%AE%83%E4%BB%AC%E5%88%86%E5%88%AB%E5%9C%A8MLP%E5%92%8CLPFP%E6%A8%A1%E5%9D%97%E4%B8%AD%E7%8B%AC%E7%AB%8B%E5%A4%84%E7%90%86%E3%80%82%E6%9D%A5%E8%87%AA%E4%B8%A4%E4%B8%AA%E6%A8%A1%E5%9D%97%E7%9A%84%E7%89%B9%E5%BE%81%E8%A2%AB%E8%BE%93%E5%85%A5%E5%88%B0%E4%B8%80%E4%B8%AATransformer%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%AD%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E3%80%82%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E8%BE%93%E5%87%BA%E8%A2%AB%E6%8F%90%E4%BE%9B%E7%BB%99%E5%85%B7%E6%9C%89%E5%A4%9A%E6%A8%A1%E6%80%81%E8%BE%93%E5%87%BA%E7%9A%84Concat%E6%A8%A1%E5%9D%97%EF%BC%88MM%EF%BC%89%E3%80%82%E5%90%8C%E6%97%B6%EF%BC%8C%E8%BF%99%E4%BA%9B%E7%89%B9%E5%BE%81%E8%A2%AB%E5%88%86%E5%88%AB%E5%8F%91%E9%80%81%E5%88%B0%E5%85%B6%E4%BB%96%E4%B8%A4%E4%B8%AATransformer%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%AD%E8%BF%9B%E8%A1%8C%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E3%80%82%E8%BF%99%E4%B8%A4%E4%B8%AATransformer%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E8%BE%93%E5%87%BA%E5%88%86%E5%88%AB%E6%98%AF%E5%85%B7%E6%9C%89%E5%9B%BE%E5%83%8F%E7%9A%84%E5%8D%95%E6%A8%A1%E6%80%81%E8%BE%93%E5%87%BA%EF%BC%88SMI%EF%BC%89%E5%92%8C%E5%85%B7%E6%9C%89%E4%BC%A0%E6%84%9F%E5%99%A8%E7%9A%84%E5%8D%95%E6%A8%A1%E6%80%81%E8%BE%93%E5%87%BA%EF%BC%88SMS%EF%BC%89%E3%80%82MM%E3%80%81SMI%E5%92%8CSMS%E7%9A%84%E7%BB%93%E6%9E%9C%E7%84%B6%E5%90%8E%E8%A2%AB%E8%BE%93%E5%85%A5%E5%88%B0Pooler%E6%A8%A1%E5%9D%97%E4%B8%AD%EF%BC%8C%E4%BB%A5%E5%87%8F%E5%B0%8F%E7%89%B9%E5%BE%81%E7%9A%84%E7%BB%B4%E5%BA%A6%E3%80%82%E6%9C%80%E5%90%8E%EF%BC%8C%E8%BF%99%E4%BA%9B%E7%89%B9%E5%BE%81%E8%A2%AB%E8%BE%93%E5%85%A5%E5%88%B0%E7%BA%BF%E6%80%A7%E5%B1%82%E6%A8%A1%E5%9D%97%EF%BC%8C%E4%BB%A5%E8%BE%93%E5%87%BA%E5%8F%B6%E9%9D%A2%E7%A7%AF%E6%8C%87%E6%95%B0%EF%BC%88LAI%EF%BC%89%E7%9A%84%E5%80%BC%EF%BC%88%E5%9C%A8%5B0%2C1%5D%E8%8C%83%E5%9B%B4%E5%86%85%EF%BC%89%E3%80%82&width=1358 "图1. ViST用于生长预测的框架。实线表示正向传递，虚线表示循环。ViST的输入是传感器和图像数据。它们分别在MLP和LPFP模块中独立处理。来自两个模块的特征被输入到一个Transformer编码器中进行特征融合。编码器的输出被提供给具有多模态输出的Concat模块（MM）。同时，这些特征被分别发送到其他两个Transformer编码器中进行自注意力机制。这两个Transformer编码器的输出分别是具有图像的单模态输出（SMI）和具有传感器的单模态输出（SMS）。MM、SMI和SMS的结果然后被输入到Pooler模块中，以减小特征的维度。最后，这些特征被输入到线性层模块，以输出叶面积指数（LAI）的值（在[0,1]范围内）。")
## 5.1 MLP模块
一般来说，图像数据具有RGB三通道，每个像素具有一个值。但传感器数据只有几十个值。因此，传感器数据的值数量远远少于图像数据。如果直接整合这两种数据类型，图像数据将占主导地位，同时传感器数据将无法很好地表现出来。为了解决这个问题，将传感器数据转化为特征图。MLP模块可以增强传感器数据的特征。传感器数据由天气数据和土壤数据组成。这些数据是数值的，共有19个数据项。在数据预处理之后，传感器数据被排列成一维向量，并输入到多层前馈感知器（MLP）模块中。MLP模块的具体结构如图2所示。在MLP模块的输出之后，生成了一个包含768个元素的一维向量，导致向量大小为1×768。然后，使用以下方程将这个向量转化为一个145×768的矩阵：
$M_{out}=W_s\times M_{in}$(1)
其中，$M_{in}$是由MLP模块生成的1x768向量，然后被转换为一个145×768的矩阵，表示为$𝑀_{𝑜𝑢𝑡}$。$𝑊_𝑠$是一个145x1矩阵，其元素是通过训练模型获得的。
![图2. 传感器输入模型结构。MLP模块是一个多层感知器。它在输入层有19个神经元，在输出层有768个神经元。两个隐藏层分别包含32和64个神经元。我们发现使用2层MLP和3层MLP在性能上没有明显差异，因此我们使用了更简单的2层MLP。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697275890712-9778a481-4bca-4d01-943a-e4520d6e326b.png#averageHue=%23fbfbfa&clientId=uc9287c01-9d87-4&from=paste&height=207&id=u17c96c05&originHeight=294&originWidth=698&originalType=binary&ratio=2&rotation=0&showTitle=true&size=30338&status=done&style=none&taskId=u8371ab3a-fa76-4d5e-9515-9f4dcf5f282&title=%E5%9B%BE2.%20%E4%BC%A0%E6%84%9F%E5%99%A8%E8%BE%93%E5%85%A5%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E3%80%82MLP%E6%A8%A1%E5%9D%97%E6%98%AF%E4%B8%80%E4%B8%AA%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%E3%80%82%E5%AE%83%E5%9C%A8%E8%BE%93%E5%85%A5%E5%B1%82%E6%9C%8919%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%8C%E5%9C%A8%E8%BE%93%E5%87%BA%E5%B1%82%E6%9C%89768%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83%E3%80%82%E4%B8%A4%E4%B8%AA%E9%9A%90%E8%97%8F%E5%B1%82%E5%88%86%E5%88%AB%E5%8C%85%E5%90%AB32%E5%92%8C64%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83%E3%80%82%E6%88%91%E4%BB%AC%E5%8F%91%E7%8E%B0%E4%BD%BF%E7%94%A82%E5%B1%82MLP%E5%92%8C3%E5%B1%82MLP%E5%9C%A8%E6%80%A7%E8%83%BD%E4%B8%8A%E6%B2%A1%E6%9C%89%E6%98%8E%E6%98%BE%E5%B7%AE%E5%BC%82%EF%BC%8C%E5%9B%A0%E6%AD%A4%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%E4%BA%86%E6%9B%B4%E7%AE%80%E5%8D%95%E7%9A%842%E5%B1%82MLP%E3%80%82&width=492 "图2. 传感器输入模型结构。MLP模块是一个多层感知器。它在输入层有19个神经元，在输出层有768个神经元。两个隐藏层分别包含32和64个神经元。我们发现使用2层MLP和3层MLP在性能上没有明显差异，因此我们使用了更简单的2层MLP。")
## 5.2 线性投影扁平化块模块
在本论文中，传感器数据被映射到与LPFP模块的图像特征相同的维度，以便后续的特征融合操作。值得注意的是，基于MLP模块的传感器特征不需要位置信息，并且传感器数据的输入顺序可以任意打乱。
![图3. 线性投影扁平化块模块。线性投影扁平化块将图像分割成若干相等的小图像，并通过矩阵变换将图像特征图与传感器特征对齐。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697281729140-0f0231f3-ec05-4c41-902c-17fcdd00067f.png#averageHue=%23f7f6f6&clientId=uc9287c01-9d87-4&from=paste&height=185&id=uf2a612b2&originHeight=339&originWidth=967&originalType=binary&ratio=2&rotation=0&showTitle=true&size=19813&status=done&style=none&taskId=ud804e491-4b49-427d-93c6-85547cd4edf&title=%E5%9B%BE3.%20%E7%BA%BF%E6%80%A7%E6%8A%95%E5%BD%B1%E6%89%81%E5%B9%B3%E5%8C%96%E5%9D%97%E6%A8%A1%E5%9D%97%E3%80%82%E7%BA%BF%E6%80%A7%E6%8A%95%E5%BD%B1%E6%89%81%E5%B9%B3%E5%8C%96%E5%9D%97%E5%B0%86%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%88%90%E8%8B%A5%E5%B9%B2%E7%9B%B8%E7%AD%89%E7%9A%84%E5%B0%8F%E5%9B%BE%E5%83%8F%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87%E7%9F%A9%E9%98%B5%E5%8F%98%E6%8D%A2%E5%B0%86%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E5%9B%BE%E4%B8%8E%E4%BC%A0%E6%84%9F%E5%99%A8%E7%89%B9%E5%BE%81%E5%AF%B9%E9%BD%90%E3%80%82&width=528.5 "图3. 线性投影扁平化块模块。线性投影扁平化块将图像分割成若干相等的小图像，并通过矩阵变换将图像特征图与传感器特征对齐。")
线性投影扁平化块模块的结构如图3所示。该模块的输入是尺寸为CHANEL×HEIGHT×WIDTH（C×H×W）的图像。这个三维张量对应于一个RGB图像。在输入图像之前，图像被调整为3×384×384的尺寸，如图3中的输入所示。通过卷积操作，图像被分割成大小为12×12的768个块。卷积操作使用大小为32×32的卷积核进行。卷积操作的步幅为32。每个小图像被展平成144个一维向量，768个展平的一维向量依次连接成768×144的向量。这个768×144的向量被转置成144×768的向量，用于与传感器的特征向量进行融合。然后，这个144×768的特征向量与类令牌向量连接在一起，生成补丁嵌入（patch embedding）。
类令牌是一个专门的令牌，可以表示图像的类别信息。通常，它被添加到图像的嵌入表示中。类令牌可以通过编码一些所有图像共享的全局特征来提供额外的信息，帮助模型更好地理解图像的内容。
1D位置嵌入的位置信息被添加到补丁嵌入中，以保留位置信息。类令牌和1D位置嵌入参考了ViT[55]。图像中每个像素的位置信息可以被转化为高维向量，其中向量的每个维度表示图像的特定维度中像素的位置。这个高维向量可以与每个像素的特征向量（例如，颜色信息、纹理信息等）连接在一起，从而得到一个更高维度的特征向量。ViT已经证明，2D位置嵌入的性能不如1D位置嵌入，因此选择了1D位置嵌入。
最后，将补丁嵌入与1D位置嵌入相加，得到图像的特征图。
图像和传感器的特征都使用相同维度的向量表示。这些向量已准备好被输入到下一个Transformer编码器中进行完全的特征融合。
#### 用于融合的 Transformer 编码器
![图4. 用于融合的Transformer编码器（TEF）模型的结构由交替层的多头交叉注意力（MHCA）、多层感知器（MLP）和层归一化（LN）模块组成。MHCA是负责融合图像和传感器特征的核心模块。MLP是从ViT中采用的，而LN模块负责执行层归一化操作。](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697282718889-93e3af57-aa42-45d9-8a92-0309a9b2e279.png#averageHue=%23e5bb69&clientId=uc9287c01-9d87-4&from=paste&height=445&id=u693dbf9d&originHeight=1236&originWidth=1582&originalType=binary&ratio=2&rotation=0&showTitle=true&size=300310&status=done&style=none&taskId=ua1cb4b6c-5abc-4b4b-b62a-64201e2d099&title=%E5%9B%BE4.%20%E7%94%A8%E4%BA%8E%E8%9E%8D%E5%90%88%E7%9A%84Transformer%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%88TEF%EF%BC%89%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%93%E6%9E%84%E7%94%B1%E4%BA%A4%E6%9B%BF%E5%B1%82%E7%9A%84%E5%A4%9A%E5%A4%B4%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88MHCA%EF%BC%89%E3%80%81%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%EF%BC%88MLP%EF%BC%89%E5%92%8C%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88LN%EF%BC%89%E6%A8%A1%E5%9D%97%E7%BB%84%E6%88%90%E3%80%82MHCA%E6%98%AF%E8%B4%9F%E8%B4%A3%E8%9E%8D%E5%90%88%E5%9B%BE%E5%83%8F%E5%92%8C%E4%BC%A0%E6%84%9F%E5%99%A8%E7%89%B9%E5%BE%81%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97%E3%80%82MLP%E6%98%AF%E4%BB%8EViT%E4%B8%AD%E9%87%87%E7%94%A8%E7%9A%84%EF%BC%8C%E8%80%8CLN%E6%A8%A1%E5%9D%97%E8%B4%9F%E8%B4%A3%E6%89%A7%E8%A1%8C%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E6%93%8D%E4%BD%9C%E3%80%82&width=569 "图4. 用于融合的Transformer编码器（TEF）模型的结构由交替层的多头交叉注意力（MHCA）、多层感知器（MLP）和层归一化（LN）模块组成。MHCA是负责融合图像和传感器特征的核心模块。MLP是从ViT中采用的，而LN模块负责执行层归一化操作。")
用于融合多模态数据的Transformer编码器（TEF）模型充当了核心模型的角色。如图4所示，TEF的计算过程如下所示。
$\begin{gathered}I_{attn},S_{attn}=MHCA\left(I_{in},S_{in}\right) \\I_{out}=MLP\left(LN\left(I_{in}\cdot I_{attn}\right)\right)+I_{in}\cdot I_{attn} \\S_{out}=MLP\left(LN\left(S_{in}\cdot S_{attn}\right)\right)+S_{in}\cdot S_{attn} \end{gathered}$
这里，$I_{𝑎𝑡𝑡𝑛}$和$S_{𝑎𝑡𝑡𝑛}$分别被定义为MHCA模块的输出。$I_{𝑎𝑡𝑡𝑛}$和$S{𝑎𝑡𝑡𝑛}$是具有注意力机制的图像和传感器特征图，分别。$𝐼_{𝑖𝑛}$和$S_{𝑖𝑛}$被分别定义为TEF的输入。$𝐼_{out}$和$S_{out}$分别是TEF的输出。$𝐼_{𝑖𝑛}$和$I_{out}$是图像特征图。$S_{𝑖𝑛}$和$S_{out}$是传感器特征图。TEF需要进行12轮的计算。TEF的输出$𝐼_{out}$和$S_{out}$将成为下一轮TEF的输入。在完成12轮后，$𝐼_{out}$和$S_{out}$将传递给下一个模块。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697283472170-c1b440e6-e2f3-498c-9ef1-14140ba68811.png#averageHue=%23fae6c1&clientId=uc9287c01-9d87-4&from=paste&height=432&id=ub3795e62&originHeight=864&originWidth=1433&originalType=binary&ratio=2&rotation=0&showTitle=false&size=104150&status=done&style=none&taskId=u1a781129-93b3-4a62-bd2f-b3cd4e44432&title=&width=716.5)
图5. 多头交叉注意力模块（MHCA）的结构充当了Transformer编码器用于融合的核心组件。它通过接受图像特征图（$𝐼_{𝑖𝑛}$）和传感器特征图（$S_{𝑖𝑛}$）作为输入进行操作。该模块使用交叉注意力机制来研究每个特征图之间以及两个特征图之间的相互关系和关联。通过这种方式，MHCA模块实现了从两个模态中识别有意义特征的改进能力，生成具有注意力的图像特征图（$I_{𝑎𝑡𝑡𝑛}$）和具有注意力的传感器特征图（$S{𝑎𝑡𝑡𝑛}$）作为输出。
交叉注意力模块被提出来建模图像和传感器特征的内部模态关系。MHCA的结构如图5所示。有两个分支用于确定参数Query（𝑄, 𝑄′）、Key（𝐾, 𝐾′）和Value（𝑉, 𝑉′）。一个分支计算了用于具有注意力的图像特征图$𝐼_{𝑎𝑡𝑡𝑛}$的𝑄, 𝐾, 𝑉。另一个分支计算了用于具有注意力的传感器特征图$𝑆_{𝑎𝑡𝑡𝑛}$的𝑄′, 𝐾′, 𝑉′。计算过程如下所示：
$\begin{gathered} Q=S_{in}W_{q},K=I_{in}W_{k},V=I_{in}W_{\upsilon} \\ A=softmax\left(\frac{QK^T}{\sqrt{C/h}}\right)V \\ Q'=I_{in}W_{q}',K'=S_{in}W_{k}',V'=S_{in}W_{v}' \\ \begin{aligned}A'&=softmax\left(\frac{Q'K'^T}{\sqrt{C/h}}\right)V'\end{aligned} \end{gathered}$
其中，$W_q,W_k,W_v,W_q^{\prime},W_k^{\prime},W_k^{\prime}\in\mathbb{R}^{C\times(C/h)}$ 是具有可学习参数的线性变换矩阵。这些多个矩阵是通过多头机制计算的。C和h分别是嵌入维度和头数。A和𝐴′分别是方程6和方程8的结果。多个A和𝐴′分别进行连接。一个分支中的传感器特征图𝑆𝑖𝑛用作查询𝑄来计算𝐾′，然后更新𝑉 ′。另一个分支中的图像特征图$𝐼_{𝑖𝑛}$用于查询𝑄′来计算键𝐾和值𝑉。因此，这两个分支在语义信息共享方面交换传感器和图像特征之间的信息。在交叉注意力中，生成注意力图的计算复杂度是线性的，而不是全注意力中的二次复杂度。MHCA模块的整个过程更加高效，可以降低过拟合的风险。
#### 损失函数
叶面积指数（LAI）数据是从农场手动收集的，用作ViST模型的标签。在数据归一化之后，LAI值被缩放到[0,1]范围。损失函数度量了模型的预测值𝑦′与实际值𝑦之间的差异。这是一个非负实值函数，通常表示为𝐿(𝑦, 𝑦′)，它构成了经验风险函数和结构风险函数的基础。
在本文中，我们将作物生长预测视为回归问题。通常，用具有单个输出节点的神经网络来解决回归问题，输出值表示预测值。为了评估我们模型的性能，我们使用均方误差损失函数作为主要指标。
$MSE\left(y,y^{\prime}\right)=\frac{\sum_{i=1}^n\left(y_i-y_i^{\prime}\right)^2}n$
其中，𝑛是样本数量，$𝑦_𝑖$是第i个样本的叶面积指数的真实值，$y_{i}^{\prime}$是第i个样本的叶面积指数的预测值。均方误差（MSE）用作我们模型输出的精度度量，用于在后续迭代中更新模型参数。
### 实验
在本节中，我们将介绍用于数据收集的方法，描述所使用的设备和数据格式。在数据收集之后，图像和传感器数据都经过预处理。还将概述模型性能的评估标准和实验细节。
#### 数据收集
数据采集的位置如图6所示。水稻、玉米和大豆的数据是在农场上收集的。每种作物有三个样本点。总共有9个样本点在农场上。数据采集设备如图7所示。农场上的每个样本点都放置了一个设备，包括一个摄像头和11个传感器。LAI数据是定期使用手持设备在每个样本点上收集的，以进行数据对齐。作物是从顶视角拍摄的。农场上摄像头捕获的水稻、大豆和玉米的图像分别如图8(a)(b)(c)所示。高度设置为3米。图像格式为RGB，分辨率为3840×2160。每张图像之间的时间间隔为两小时。所收集的传感器项目如表1所示，包括十一个传感器数据类型。土壤传感器分别部署在地下的10cm、20cm、30cm、40cm和50cm深度。空气、光线和风速传感器部署在IoT设备的顶部。每个传感器的数据收集时间间隔为半小时。图像和传感器数据定期收集并上传到云服务器进行存储和数据分析。
#### 数据处理
**图像数据的预处理**：为了确保在反向传播中更好的收敛性，对每幅图像的数据使用Z分数方法进行归一化，将数据归一化到特定范围。

$z=\frac{x-\mu}\sigma$

$z=\frac{x-\mu}\sigma$
在本研究中，其中𝜇表示均值，𝜎表示标准差，𝑥表示输入数据，𝑧表示输出数据。分别记录了图像三个通道的均值和标准差。具体来说，将规范化图像的每个通道的均值设置为0，方差设置为1。但需要注意的是，这种归一化方法可能不适用于样本量较小的情况，一般建议仅在样本量超过30时使用。表2展示了水稻、大豆和玉米图像计算的规范化结果。
**传感器数据的预处理**：为了消除尺寸之间的影响，我们使用以下方程对每个尺寸进行了归一化，因为传感器数据中各尺寸之间的数值差异相对较大。
$x_{normalized}=\frac{x-x_{min}}{x_{max}-x_{min}}$
其中，$x_{min}$是最小值，$x_{max}$是最大值。$𝑥_{𝑛𝑜𝑟𝑚𝑎𝑙𝑖𝑧𝑒𝑑}$是归一化后的数值。
当用于处理实际数据时，每个指标的最大值和最小值被保存在一个单独的文件中，用于数据预处理。处理后的数据如表3所示。
表2. 图像标准化数据。该表呈现了三种作物（水稻、大豆和玉米）的图像RGB通道的均值和标准差值。这些值是通过前面部分描述的规范化过程获得的。
![CleanShot 2023-10-14 at 20.21.31@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697286099049-834f6412-b9fc-4440-adfd-8779140b1581.png#averageHue=%23efefef&clientId=uc9287c01-9d87-4&from=paste&height=330&id=uf94e93f3&originHeight=744&originWidth=1124&originalType=binary&ratio=2&rotation=0&showTitle=false&size=110974&status=done&style=none&taskId=u8c448331-63f2-4d28-8d67-cfc3f1e6a6c&title=&width=498)
表3. 传感器输入指标的预处理。该表包括传感器数据处理的指标，以及它们的最大和最小值。
![CleanShot 2023-10-14 at 20.41.07@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697287276999-595397e1-3dc5-445e-9e6a-9a9ee31f386f.png#averageHue=%23ededed&clientId=uc9287c01-9d87-4&from=paste&height=548&id=uf474c586&originHeight=1204&originWidth=1350&originalType=binary&ratio=2&rotation=0&showTitle=false&size=263018&status=done&style=none&taskId=u9ae5c964-8960-4a70-9e2e-75954fdd759&title=&width=615)
**LAI数据的处理：**为了确保在反向传播中更好的收敛性，使用Z分数方法对图像数据进行了归一化，将数据归一化到特定范围。对于每种作物，三组图像和传感器设备放置在测量LAI时与相机的相同位置。在LAI数据收集过程中，手持设备捕捉相机设备覆盖的部分区域。LAI数据每5天收集一次。为了获取更多的数据，使用了分段三次埃尔米特插值多项式(piecewise cubic Hermite interpolation polynomial (PCHIP) )方法，如Fritsch和Carlson[56]所述。LAI数据每天都可用，并对水稻、大豆和玉米的三个样本点进行插值，如图9(a),(b),(c)所示。图的左侧显示了原始数据，右侧显示了插值数据。插值数据看起来更平滑，从曲线中可以看出。
![水稻LAI插值前后](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697287432759-e366d76b-1105-4900-a91f-6c283f991fea.png#averageHue=%23faf7f6&clientId=uc9287c01-9d87-4&from=paste&height=328&id=ub764d341&originHeight=656&originWidth=2026&originalType=binary&ratio=2&rotation=0&showTitle=true&size=293800&status=done&style=none&taskId=u00b8a312-d839-45cf-a711-e0a953049e1&title=%E6%B0%B4%E7%A8%BBLAI%E6%8F%92%E5%80%BC%E5%89%8D%E5%90%8E&width=1013 "水稻LAI插值前后")
![大豆LAI插值前后](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697287448877-72b6bdb8-345e-40bd-90dc-e0d8ee44c91d.png#averageHue=%23f9f7f2&clientId=uc9287c01-9d87-4&from=paste&height=308&id=uf7bc0c0a&originHeight=616&originWidth=2036&originalType=binary&ratio=2&rotation=0&showTitle=true&size=271511&status=done&style=none&taskId=u7b1db0ec-f748-4486-b2c6-65bffc4669a&title=%E5%A4%A7%E8%B1%86LAI%E6%8F%92%E5%80%BC%E5%89%8D%E5%90%8E&width=1018 "大豆LAI插值前后")
![玉米LAI插值前后](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697287457482-287c10c8-9a10-4f2d-b62e-26e8fd6615d1.png#averageHue=%23fafafa&clientId=uc9287c01-9d87-4&from=paste&height=302&id=ub42440ee&originHeight=604&originWidth=2104&originalType=binary&ratio=2&rotation=0&showTitle=true&size=226060&status=done&style=none&taskId=u4bb5ca97-af44-43ee-a356-989fa32b50e&title=%E7%8E%89%E7%B1%B3LAI%E6%8F%92%E5%80%BC%E5%89%8D%E5%90%8E&width=1052 "玉米LAI插值前后")
图9. LAI数据预处理。图中显示了插值前后的叶面积指数数据。横轴表示时间，纵轴表示叶面积指数。
**数据对齐：**一个样本包括图像数据、传感器数据和一个标签 - LAI数据。基于各自的采样位置和时间，对图像数据、传感器数据和LAI数据进行了对齐。每幅图像是以两小时的间隔捕获的，而传感器的采样频率为每半小时一次。数据集之间的时间对齐是以半小时为基础建立的。由于相对于传感器数据，图像数据的数量较少，因此在两小时内收集的四个样本对应一幅图像。值得注意的是，每天只有一个LAI数据点；这意味着有四十八个样本共享相同的LAI标签。
训练集和测试集：对于每种作物，选择了三个样本点，在农场上共有九个观测点。其中两个样本点用于模型训练，而剩余的一个样本点用于模型测试。针对每种作物，跨不同样本点进行交叉验证，以防止在训练和测试中使用相同的样本数据，这可能导致数据梯度的差异较小。
实验中使用的数据量如图4所示，其中“days”表示数据收集的持续时间。“Train sample”和“Test sample”分别指的是用于训练和测试的数据实例数量。
表4. 实验数据的数量。
![CleanShot 2023-10-14 at 20.48.47@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697287734540-372eb2f4-6982-47d1-bcc1-36c61f548a2e.png#averageHue=%23ededed&clientId=uc9287c01-9d87-4&from=paste&height=167&id=u8ee41ba2&originHeight=334&originWidth=978&originalType=binary&ratio=2&rotation=0&showTitle=false&size=43275&status=done&style=none&taskId=uf46d4c1d-41ec-48de-9094-04619605b46&title=&width=489)

#### 评估指标
均方误差（MSE）、平均绝对误差（MAE）、平均绝对百分比误差（MAPE）和对称平均绝对百分比误差（SMAPE）用于预测各种模型的性能。MSE与损失函数相同。MAE是目标值和预测值之间绝对差的总和。
$MAE=\frac{\sum_i^n\left|y_i-y_i^{\prime}\right|}n$
其中，$y_i^{\prime}$是模型的评估值，$𝑦_𝑖$是真实值，𝑛是样本总数。均方误差（MSE）是模型评估值与实际样本值𝑦之间差的平方的平均值。其公式如下：
$MSE=\frac{\sum_{i=1}^n\left(y_i^{\prime}-y_i\right)^2}n$
其中，$𝑦_𝑖$和$y_i^{\prime}$，i分别是第一个样本的真实值和相应的评估值，𝑛是样本数量。MAPE是估计中常用的性能度量标准。它通过计算估计值$y_i^{\prime}$与实际值$𝑦_𝑖$之间的绝对百分比差异来衡量估计的准确性，然后取所有绝对百分比差异的平均值。计算MAPE的公式如下：
$MAPE=\frac{100\%}n\sum_{i=1}^n\left|\frac{y_i^{\prime}-y_i}{y_i}\right|$
其中，𝑛是样本数量，$𝑦_𝑖$是实际值，$y_i^{\prime}$是估计值。SMAPE是评估预测模型准确性的广泛使用的度量标准。它是一种对称误差，计算实际值和评估值之间的平均百分比差异。与其他误差度量不同，SMAPE考虑了实际值的幅度，使其成为比较不同预测模型准确性的更可靠的度量标准。计算SMAPE的公式如下：
$SMAPE=\frac{100\%}n\sum_{i=1}^n\frac{\left|y_i^{\prime}-y_i\right|}{\left(\left|y_i^{\prime}\right|+\left|y_i\right|\right)/2}$
其中，𝑛、$𝑦_𝑖$和$y_i^{\prime}$与MAPE公式中的符号相同。SMAPE的范围从0%到200%，数值越低表示准确性越好。
#### 实验和结果
在这一部分，我们进行了广泛的实验，以展示我们提出的ViST模型相对于现有方法的有效性。我们的模型与最先进的模型进行了比较，如Rice-Fusion [57]、RiceTransformer [58]、CNN-Transformer [59]、DNNF1和DNNF2 [49]。虽然DNNF1和DNNF2都是用于大豆产量预测的多模态模型，但Rice-Fusion和Rice-Transformer采用了多模态方法来诊断水稻疾病。CNN-Transformer模型是基于多时相数据对作物进行分类的。在比较方面，这些模型的输入和输出与我们的ViST模型相同。最后，所有模型的输出都基于代表作物生长状态的LAI值进行评估。
ViST模型进行了去除实验，涉及其输入模式，包括仅图像数据、仅传感器数据和多模态数据。使用一系列指标对每个模型的性能进行了评估，包括MAE、MSE、MAPE和SMAPE，它们分别在方程12、13、14和15中呈现。
预处理后的数据被用来训练每个模型，使用了ViST的主要超参数，这些超参数列在表5中。采用了Adam优化器，权重衰减为0.0001，并且所有模型都在GeForce RTX 3090 GPU上进行训练。为了动态调整学习率，实施了余弦退火策略。最大迭代次数是根据样本大小和时代确定的，在训练过程中，学习率随着时代的增加而单调下降。ViST模型的主要超参数也可以在表5中找到。使用随机正态分布来初始化每个模型的参数。在去除实验中，MM、SMI和SMS被用作模型的输入。
表5. 模型的主要超参数
![CleanShot 2023-10-14 at 21.05.18@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697288724954-b9fde838-08b4-402f-93c7-878281d7d495.png#averageHue=%23eeeeee&clientId=uc9287c01-9d87-4&from=paste&height=349&id=u09103137&originHeight=698&originWidth=662&originalType=binary&ratio=2&rotation=0&showTitle=false&size=69099&status=done&style=none&taskId=u581d03d0-8403-4269-b05a-efb0166ff6c&title=&width=331)
##### 针对单一作物的实验和结果
在本节中，我们评估了我们的模型在三种作物——水稻、大豆和玉米上的性能。首先，我们比较了使用单模态数据和多模态数据的模型性能，如A节中所讨论的。其次，我们将我们的ViST模型的性能与其他模型的性能进行了比较，如B节中所概述的。
**A. ViST模型在单模态数据和多模态数据下的性能比较**
ViST模型针对每种作物使用了三种不同的数据输入模式进行训练：图像数据、传感器数据、图像和传感器数据的组合。因此，每种作物都得到了三个训练好的模型。
表6. 不同输入模式下的ViST模型测试结果。
![CleanShot 2023-10-14 at 22.09.06@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697292553951-819fe5b4-5bc2-491b-90e9-abcb04abfd6d.png#averageHue=%23e6e6e6&clientId=uc9287c01-9d87-4&from=paste&height=291&id=u53f3c525&originHeight=708&originWidth=1424&originalType=binary&ratio=2&rotation=0&showTitle=false&size=141836&status=done&style=none&taskId=ua50200a4-f5d5-4877-86fc-d5ba348ece1&title=&width=585)
在获得训练模型之后，我们使用测试数据集中的数据来测试这些模型。ViST模型在测试数据集上的性能如表6所示。
结果显示，在三种作物的三种模式中，多模态模式在预测指标方面取得了最低值。由于四个指标在实验结果上呈现相同趋势，因此采用MAE来分析ViST模型的性能。在水稻实验中，与传感器和图像模式相比，MAE值分别减少了61.25%、74.49%。在大豆实验中，MAE值分别减少了97.13%、92.94%。玉米实验中，MAE值分别减少了90.45%、93.35%。因此可以得出结论：将图像和传感器数据结合起来显著提高了ViST模型的性能，并且传感器和图像数据对于模型训练具有几乎相等的贡献。
对于水稻而言，传感器比图像更有效果，因为水稻生长过程中图像变化很小且叶片特征不明显；对于大豆而言，则是图像比传感器更有效果，因为其生长过程中图像特征发生明显变化；至于玉米，则是图像和传感器数据差异不大，两者都能有效反映作物生长情况，因为各种评估指标的误差率相对较小。
利用图像分析可以分析作物表面纹理特征，如颜色、质地和形状，以推断作物的健康状况，包括是否生长良好以及是否受到害虫和疾病的影响。传感器数据可以捕捉环境因素（如温度、湿度、光线和土壤）的变化，并评估其对作物生长的影响。例如，过度干旱或过度湿润会对作物生长产生负面影响。通过综合分析这两个方面的信息，可以更准确地评估作物生长情况，并相应调整适当的农业管理措施。
在水稻、大豆和玉米实验中，多模态结果优于单模态结果。多模态数据可以补充单模态数据中缺失的信息，填补信息空白。例如，图像只能反映作物表面信息，而传感器只能反映环境信息。如果仅使用单一模态数据，则会忽略许多重要信息。通过整合多种类型的信息，在数据分析方面不同类型的信息可以互补彼此，提高准确性和全面性。
图像和传感器多模态数据的融合可以完成噪声抑制。数据上的噪声是常见的，尤其在传感器数据处理中。然而，通过融合数据，多模态数据可以减少数据上的噪声，这比单一模态数据分析更具鲁棒性。
单一模态数据比多模态数据更容易受到干扰，后者更具鲁棒性。传感器数据通常会受到各种外部环境因素（如雨水、灰尘和遮挡）的影响，这可能会影响到数据准确性。相反，图像数据受外部因素的影响较小。因此，在多模态 数据分析中，图像 数据与其他类型 的组合显著提高了数据准确性和鲁棒性。
因此，多模态数据分析的互补性、噪声抑制和鲁棒性可以更全面地理解物体的本质并更准确地描述实际情况。
**B. 与其他模型的比较**
在本节中，我们将提出的ViST模型与其他模型进行了比较：DNNF1、DNNF2、CNN-Transformer、Rice-Fusion和Rice-Transformer。这三个模型分别在稻谷、大豆和玉米数据上进行了训练。
表7：水稻比较实验测试结果。“平均值”和“标准差”分别表示从五个实验组得到的结果的平均值和标准差。
![CleanShot 2023-10-14 at 22.35.53@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697294164487-0ae746f9-6606-4633-929d-35786c459625.png#averageHue=%23e6e6e6&clientId=ue8499c17-ada4-4&from=paste&height=115&id=u1f9d4810&originHeight=230&originWidth=958&originalType=binary&ratio=2&rotation=0&showTitle=false&size=55906&status=done&style=none&taskId=u5c154b00-afaa-4c7f-b1d6-0b89aa8a4d8&title=&width=479)
表7显示了将各种模型应用于稻谷数据集时的实验结果。ViST模型在所有预测指标上都展现出最佳性能，这意味着它在估计稻谷生长方面具有出色的表现。平均而言，ViST将MSE降低了55.80％，MAE降低了38.48％，MAPE降低了44.21％，SMAPE降低了42.59％。与其他模型相比，ViST模型在准确性和稳定性方面表现更好，尤其是与DNNF1和DNNF2相比较。例如，与DNNF1相比，ViST将MSE降低了87.01％、MAE降低了48.00％、MAPE降低了61.83％、SMAPE降低了56.24％。同时还显示出ViST在MSE上几乎与CNN-Transformer和Rice-Fusion模型具有相同的值。
与表7类似，表8展示了ViST和其他比较模型的实验结果。表8中的实验使用大豆数据作为数据集。通过考虑相对于其他模型的改进比率，可以明显看出ViST在所有四个指标上都取得了显著的提升，其中在MAE和MAPE指标上改进最为明显。关于MSE指标，相对于DNNF1、DNNF2、Rice-Fusion和Rice-Transformer，ViST分别实现了56.25%、92.31%、61.11%和50.00%的减少。此外，相对于CNN-Transformer，ViST将误差降低了46.15%。
表9显示了将各种模型应用于玉米数据集时的实验结果。Rice-Fusion模型在四个性能指标上表现良好，尽管在MAPE
表8方面略逊于ViST。大豆的比较实验测试结果。“Mean”和“Std”分别表示从五个实验组得到的结果的平均值和标准偏差。
![CleanShot 2023-10-14 at 22.51.20@2x.png](https://cdn.nlark.com/yuque/0/2023/png/25721528/1697295099495-9c8c1576-6c55-49bf-8770-d5394ffca0ac.png#averageHue=%23e7e7e7&clientId=ue8499c17-ada4-4&from=paste&height=119&id=ucd4da0e7&originHeight=238&originWidth=976&originalType=binary&ratio=2&rotation=0&showTitle=false&size=57145&status=done&style=none&taskId=ue42caeae-57e0-49ed-ac0c-803024bd453&title=&width=488)
表9. 玉米的比较实验测试结果。"平均值"和"标准差"分别表示从五个实验组得到的结果的平均值和标准差。