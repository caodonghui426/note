# 1.摘要
我们提出了一个统一的视觉-语言预训练模型（VLMO），它通过模块化Transformer网络共同学习双编码器和融合编码器。具体而言，我们引入了混合模态专家（MOME）Transformer，在每个块中包含一组特定于模态的专家和一个共享的自注意层。由于MOME的建模灵活性，预训练的VLMO可以作为用于视觉-语言分类任务的融合编码器进行微调，或者作为高效图像-文本检索的双编码器使用。此外，我们提出了一种分阶段预训练策略，有效利用大规模仅图像和仅文本数据以及图像-文本对。实验结果表明，VLMO在各种视觉-语言任务上取得了最先进的结果，包括VQA、NLVR2和图像-文本检索。代码和预训练模型可在https://aka.ms/vlmo获取。
# 2.结论
在这项工作中，我们提出了一个统一的视觉-语言预训练模型VLMO，它通过共享的MOME Transformer骨干网络同时学习双编码器和融合编码器。MOME引入了一组模态专家来编码特定于模态的信息，并使用共享的自注意力模块对不同的模态进行对齐。与MOME一起进行的统一预训练使得该模型可以用作高效视觉-语言检索的双编码器，或者用作融合编码器以建模跨模态交互用于分类任务。我们还展示了分阶段预训练利用大规模图像和文本语料库极大地改善了视觉-语言预训练。实验结果表明，在各种视觉-语言分类和检索基准上，VLMO优于先前最先进的模型。
未来，我们希望从以下几个方面改进VLMO：
- 我们将扩大VLMO预训练中使用的模型规格。
- 我们也有兴趣根据UniLM [11]中提出的方法对VLMO进行微调，以应用于视觉-语言生成任务（如图像字幕）。
- 我们将探索在多大程度上视觉-语言预训练可以相互帮助每个模态，特别是由于共享的MOME骨干网络自然地融合了文本和图像表示。
- 我们可以扩展所提出的模型以集成更多的模态（例如语音、视频和结构化知识），支持通用多模态预训练。
# 3.引言
视觉语言（VL）预训练[30, 41, 35, 26, 20, 23]从大规模的图像-文本对中学习通用的跨模态表示。以往的模型通常采用图像-文本匹配、图像-文本对比学习、掩码区域分类/特征回归、词-区域/补丁对齐和掩码语言建模等方法来聚合和对齐视觉和语言信息。然后，预训练模型可以直接在下游视觉语言任务上进行微调，例如VL检索和分类（视觉问答，视觉推理等）。
先前工作中广泛使用了两种主流架构。CLIP [35] 和ALIGN [18]采用双编码器架构分别编码图像和文本。通过计算图像和文本特征向量之间的余弦相似度来处理多样性交互。双编码器架构在检索任务中非常有效，尤其是针对大量的图像和文本数据。可以预先计算并存储图像和文本的特征向量。然而，图片与文字之间浅层次的交互不足以处理复杂的VL分类任务。ViLT [20]发现CLIP在视觉推理任务上准确率相对较低。另一方面的工作[30, 41, 43, 3, 20, 23]依赖于融合编码器和跨模态注意力来建模图像-文本对。通常使用多层Transformer [45]网络来融合图像和文本表示。融合编码器架构在VL分类任务上取得了优越的性能。但是，它需要联合编码所有可能的图像-文本对以计算检索任务的相似度分数。二次时间复杂度导致推理速度比时间复杂度为线性的双编码器模型要慢得多。
为了充分利用这两种类型的架构，我们提出了一个统一的视觉语言预训练模型（VLMO），可以用作双编码器，将图像和文本分别进行编码以进行检索任务；或者用作融合编码器，以建模图片与文字之间深入交互进行分类任务。这通过引入Mixture-of-Modality-Experts（MOME）Transformer实现，该Transformer可以在一个Transformer块内对各种形式（图像、文本和图像-文本对）进行编码处理。MOME采用一组形式专家代替标准Transformer中的前馈网络，并通过切换到不同形式专家捕捉特定于形式的信息，并使用跨形式共享自注意力来对齐视觉和语言信息。具体而言，MOME Transformer由三个形式专家组成，即用于图像编码的视觉专家、用于文本编码的语言专家以及用于图像-文本融合的视觉-语言专家。由于建模灵活性，我们可以重复使用具有共享参数的MOME Transformer进行不同目的，例如仅文本编码器、仅图像编码器和图像-文本融合编码器。
VLMO是通过三个预训练任务共同学习的，即图像-文本对比学习、图像-文本匹配和掩码语言建模。此外，我们提出了一种分阶段预训练策略，以有效利用大规模的仅图像和仅文本语料库，并结合VLMO预训练中的图像-文本对。我们首先使用BEIT [2]中提出的掩码图像建模方法，在仅有图像数据上对视觉专家和自注意力模块进行预训练。然后，我们使用掩码语言建模[10]在仅有文本数据上对语言专家进行预训练。最后，该模型被用于初始化视觉-语言预训练过程。通过摆脱受限制大小的图像-文本对及其简单短小的标题，分阶段地在大量仅图像和仅文本数据上进行预训练有助于VLMO学习更具泛化能力的表示形式。
实验结果表明，VLMO在视觉-语言检索和分类任务上取得了最先进的结果。作为双编码器使用时，我们的模型优于基于融合编码器的模型[3, 14, 20, 23] ，同时在检索任务中拥有更快速度推理速度。此外，我们的模型在视觉问答（VQA）和用于视觉推理的自然语言（NLVR2）任务上也取得了最先进的结果，其中VLMO被用作融合编码器。
我们的主要贡献总结如下：
- 我们提出了一个统一的视觉-语言预训练模型VLMO，可用作分类任务的融合编码器，或者作为检索任务的双重编码器进行微调。
- 我们引入了一个通用的多模态Transformer用于视觉-语言任务，即MOME Transformer，以对不同形式进行编码。它通过形式专家捕捉特定于形式的信息，并通过跨形式共享自注意力模块来对不同形式内容进行对齐。
- 我们证明了分阶段使用大量仅图像和仅文本数据进行预训练极大地改善了我们的视觉-语言预训练模型。
# 4.相关工作
使用Transformer [45]骨干网络进行预训练，显著提升了自然语言处理[34, 10, 28, 22, 11, 36, 1, 7 ,8 ,4 –6 ,31 ]、计算机视觉[12,44 ,2]和视觉-语言[43,41 ,3 ,49 ,35 ,18 ,20 ,23 ]任务的最新技术水平。
视觉-语言预训练方法可以分为两类。第一类利用双编码器分别对图像和文本进行编码，并使用余弦相似度或线性投影层来建模图像和文本之间的交互作用[35 ，18 ]。通常采用图像-文本对比学习来优化模型。双编码器模型在视觉-语言检索任务中非常有效。然而，简单的交互不足以处理需要复杂推理的任务，例如视觉推理和视觉问答（VL分类任务）。第二类通过跨模态注意力使用深度融合编码器来建模图像和文本之间的交互作用[43 ，30 ，41 ，24 ，51 ，3 ，26 ，25，14，49，16，17，20，23，46] 。广泛使用图像-文本匹配、掩蔽式语言建模、词区域/补丁对齐、掩蔽式区域分类和特征回归来训练基于融合编码器的模型。这些模型在视觉-语言分类任务中取得了更好的性能，但是对所有图像-文本对进行联合编码会导致检索任务的推理速度变慢。大部分基于融合编码器的模型依赖现成的目标检测器（如Faster R-CNN [37]）获取图像区域特征。生成区域特征会降低推理速度，并使方法不太可扩展。最近，Pixel-BERT [16]去除了目标检测器，并通过卷积神经网络将图像编码为网格特征。ALBEF [23]使用图像Transformer[12,44 ]获取图像表示，并使用文本Transformer[10 ]学习上下文化表示形式，然后通过跨模态注意力进行融合。ViLT [20]将图像编码为补丁嵌入向量，然后将图像补丁嵌入向量和词嵌入向量串联输入到Transformer网络中学习上下文化表示并建模图像和文本之间的交互作用。
与以前的工作不同，我们使用共享MOME Transformer进行统一预训练，使得该模型可以针对检索任务执行单独编码，并联合对所有图片-文本对进行编码以捕捉更深层次的交互作用，我们的模型在检索和分类任务中实现了竞争性能，并享受更快的推理速度。
# 5.方法
通过MOME Transformer网络，给定图像-文本对，VLMO获取图像独立表示、文本独立表示以及图像-文本对表示。如图1所示，统一预训练优化共享的MOME Transformer，在仅有图像和仅有文本的表示上进行图像-文本对比学习，在图像-文本对表示上进行图像-文本匹配和掩码语言建模。由于建模灵活性，该模型可以用作检索任务的双编码器，在微调期间分别编码图片和文字。它还可以作为融合编码器进行微调，以更深入地建模图片和文字之间的多模态交互，并用于分类任务。